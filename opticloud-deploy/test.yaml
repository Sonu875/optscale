---
# Source: opticloud/templates/mongodb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: "mongo"
  labels:
    app: "mongo"
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: "mongo"
      release: "release-name"
---
# Source: opticloud/templates/rabbitmq.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: "rabbitmq"
  labels:
    app: "rabbitmq"
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: "rabbitmq"
      release: "release-name"
---
# Source: opticloud/templates/rabbitmq.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: rabbitmq
    release: release-name
  name: rabbitmq
automountServiceAccountToken: true
---
# Source: opticloud/templates/mariadb.yaml
apiVersion: v1
kind: Secret
metadata:
  name: mariadb-secret
type: Opaque
data:
  password: bXktcGFzc3dvcmQtMDE=
---
# Source: opticloud/templates/minio.yaml
apiVersion: v1
kind: Secret
metadata:
  name: "minio-secret"
type: Opaque
data:
  access: "b3B0c2NhbGUtbWluaW8="
  secret: "c2VjcmV0X3Bhc3N3b3Jk"
---
# Source: opticloud/templates/mongodb.yaml
apiVersion: v1
kind: Secret
metadata:
  name: "mongo-secret"
  labels:
    app: mongo
type: Opaque
data:
  username: cm9vdA==
  password: U2VjdXJlUGFzc3dvcmQtMDEtMDI=
  key.txt: c2VjdXJlU2hhcmRpbmdLZXlGRkZERGExMjk=
---
# Source: opticloud/templates/rabbitmq.yaml
apiVersion: v1
kind: Secret
metadata:
  name: rabbit-secret
  labels:
    app: rabbitmq
    release: "release-name"
type: Opaque
data:
  username: "b3B0c2NhbGU="
  password: "c2VjdXJlLXBhc3N3b3JkLWhlcmU="
  management-username: "b3B0c2NhbGU="
  management-password: "c2VjdXJlLXBhc3N3b3JkLWhlcmU="
  erlang-cookie: "ZFdraFhlaE9BcEgxT0hzaHZCeWZwZlRmM0NkOHd3a0FEZWk3bVdkNUpG"
  definitions.json: "ewogICJwb2xpY2llcyI6IFt7CiAgICAibmFtZSI6ICJoYS1hbGwiLAogICAgInBhdHRlcm4iOiAiLioiLAogICAgInZob3N0IjogIi8iLAogICAgImRlZmluaXRpb24iOiB7CiAgICAgICJoYS1tb2RlIjogImFsbCIsCiAgICAgICJoYS1zeW5jLW1vZGUiOiAiYXV0b21hdGljIiwKICAgICAgImhhLXN5bmMtYmF0Y2gtc2l6ZSI6IDEKICAgIH0KICB9XQp9"
---
# Source: opticloud/templates/report_import_scheduler.yaml
apiVersion: v1
kind: Secret
metadata:
  name: cluster-secret
  labels:
    app: report-import-scheduler
    release: "release-name"
type: Opaque
data:
  cluster_secret: "dGVzcg=="
---
# Source: opticloud/templates/thanos_receive.yaml
apiVersion: v1
kind: Secret
metadata:
  name: thanos-secret
  labels:
    app: thanos-receive
    release: release-name
type: Opaque
stringData:
  thanos_conf.yaml: |
    type: S3
    config:
      bucket: thanos
      endpoint: "minio:80"
      access_key: "opticloud-minio"
      secret_key: "secret_password"
      insecure: true
    prefix: "data"
---
# Source: opticloud/templates/clickhouse/configmap.yaml
kind: ConfigMap
apiVersion: v1
data:
  config.xml: |-
    <?xml version="1.0"?>
    <yandex>
        <logger>
            <console>1</console>
        </logger>
        <http_port>8123</http_port>
        <tcp_port>9000</tcp_port>
        <listen_host>0.0.0.0</listen_host>
        <max_connections>4096</max_connections>
        <keep_alive_timeout>3</keep_alive_timeout>
        <max_concurrent_queries>100</max_concurrent_queries>
        <uncompressed_cache_size>8589934592</uncompressed_cache_size>
        <mark_cache_size>5368709120</mark_cache_size>
        <path>/var/lib/clickhouse/</path>
        <tmp_path>/var/lib/clickhouse/tmp/</tmp_path>
        <users_config>users.xml</users_config>
        <default_profile>default</default_profile>
        <default_database>default</default_database>
        <zookeeper incl="zookeeper-servers" optional="true" />
        <macros incl="macros" optional="true" />
        <builtin_dictionaries_reload_interval>3600</builtin_dictionaries_reload_interval>
        <max_session_timeout>3600</max_session_timeout>
        <default_session_timeout>60</default_session_timeout>
        <query_log>
            <database>system</database>
            <table>query_log</table>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </query_log>
        <dictionaries_config>*_dictionary.xml</dictionaries_config>
        <resharding>
            <task_queue_path>/clickhouse/task_queue</task_queue_path>
        </resharding>
        <distributed_ddl>
            <path>/clickhouse/task_queue/ddl</path>
        </distributed_ddl>
    </yandex>
  users.xml: |-
    <?xml version="1.0"?>
    <yandex>
        <profiles>
            <default>
                <max_memory_usage>10000000000</max_memory_usage>
                <use_uncompressed_cache>0</use_uncompressed_cache>
                <load_balancing>random</load_balancing>
            </default>
            <readonly>
                <readonly>1</readonly>
            </readonly>
        </profiles>
        <users>
            <default>
                <password>secure-password-1-clk</password>
                <networks incl="networks" replace="replace">
                    <ip>::/0</ip>
                </networks>
                <profile>default</profile>
                <quota>default</quota>
            </default>
        </users>
        <quotas>
            <default>
                <interval>
                    <duration>3600</duration>
                    <queries>0</queries>
                    <errors>0</errors>
                    <result_rows>0</result_rows>
                    <read_rows>0</read_rows>
                    <execution_time>0</execution_time>
                </interval>
            </default>
        </quotas>
    </yandex>
metadata:
  name: clickhouse-config
  labels:
    addonmanager.kubernetes.io/mode: Reconcile
    heritage: Helm
    release: release-name
    app: clickhouse
---
# Source: opticloud/templates/etcd.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: opticloud-etcd
data:
  config: "skip_config_update: false\ndrop_tasks_db: \ndatabases:\n  - \"auth-db\"\n  - \"my-db\"\n  - \"tasks\"\n  - \"herald\"\n  - \"katara\"\n  - \"slacker\"\n  - \"jira-bus\"\n# initial key-value pairs for etcd\n# structure\n#\n#   branch:\n#     key: value\n#\n# became /branch/key = value in etcd\n# lists are handled with append=true, so\n#\n# list:\n#   - value\n#\n# will look like\n# /list/0000000022 = value\netcd:\n  public_ip: \n  encryption_key: \n  release: \n  katara_scheduler_timeout: 3600\n  bumi_scheduler_timeout: 300\n  bumi_worker:\n    max_retries: 5\n    wait_timeout: 7200\n    task_timeout: 3600\n    run_period: 10800\n  opticloud_service_emails:\n    recipient: \n    enabled: false\n  opticloud_error_emails:\n    recipient: \n    enabled: false\n  bulldozer_worker:\n    max_retries: 5\n    wait_timeout: 7200\n    task_timeout: 3600\n    run_period: 10800\n  google_calendar_service:\n    enabled: false\n    access_key:\n        auth_provider_x509_cert_url: \n        auth_uri: \n        client_email: \n        client_id: \n        client_x509_cert_url: \n        private_key: \n        private_key_id: \n        project_id: \n        token_uri: \n        type: \n  domains_blacklists:\n    new_employee_email:\n    registration:\n    failed_import_email:\n  domains_whitelists:\n    new_employee_email:\n    registration:\n    failed_import_email:\n  secret:\n    cluster: tesr\n    agent: \n  images_source:\n    host: \n    tag: \n  restapi:\n    invite_expiration_days: 30\n    host: restapi\n    port: 80\n    demo:\n      multiplier: 2\n  auth:\n    host: auth\n    port: 80\n  katara:\n    host: katara\n    port: 80\n  herald:\n    host: herald\n    port: 80\n  keeper:\n    host: keeper\n    port: 80\n  insider:\n    host: insider-api\n    port: 80\n  slacker:\n    host: slacker\n    port: 80\n  jirabus:\n    host: jira-bus\n    port: 80\n  arcee:\n    host: arcee\n    port: 80\n  bulldozer_api:\n    host: bulldozer-api\n    port: 80\n  metroculus:\n    host: metroculusapi\n    port: 80\n  thanos_query:\n    host: thanos-query\n    port: 10902\n  thanos_receive:\n    host: thanos-receive\n    port: 19291\n    path: api/v1/receive\n  authdb:\n    host: mariadb\n    user: root\n    password: my-password-01\n    db: auth-db\n  heralddb:\n    host: mariadb\n    user: root\n    password: my-password-01\n    db: herald\n  restdb:\n    host: mariadb\n    user: root\n    password: my-password-01\n    db: my-db\n    port: 3306\n  kataradb:\n    host: mariadb\n    user: root\n    password: my-password-01\n    db: katara\n  slackerdb:\n    host: mariadb\n    user: root\n    password: my-password-01\n    db: slacker\n    port: 3306\n  jirabusdb:\n    host: mariadb\n    user: root\n    password: my-password-01\n    db: jira-bus\n    port: 3306\n  mongo:\n    host: mongo\n    port: 80\n    user: root\n    pass: SecurePassword-01-02\n    database: keeper\n  influxdb:\n    host: influxdb\n    port: 80\n    user:\n    pass:\n    database: metrics\n  rabbit:\n    user: opticloud\n    pass: secure-password-here\n    host: rabbitmq\n    port: 5672\n  minio:\n    host: minio\n    port: 80\n    access: opticloud-minio\n    secret: secret_password\n  clickhouse:\n    host: clickhouse\n    port: 9000\n    user: default\n    password: secure-password-1-clk\n    db: default\n  cleanmongodb:\n    chunk_size: 500\n    rows_limit: 10000\n    archive_enable: \n    file_max_rows: 10000\n  encryption_salt: \n  encryption_salt_auth: \n\n  certificates:\n    opticloud: \n\n  logstash_port: \n  events_queue: events_queue\n  resources_discovery_cache_time: \n  overlay_list: \n  token_expiration: 168\n  users_dataset_generator:\n    enable: false\n    bucket: \n    s3_path: \n    filename: \n    aws_access_key_id: \n    aws_secret_access_key: \n  service_credentials:\n    null\n  opticloud_meter_enabled: 0\n\n  smtp:\n    server: \n    email: \n    port: \n    password: \n  resource_discovery_settings:\n    discover_size: 10000\n    timeout: \n    writing_timeout: 60\n    observe_timeout: 7200\n  bi_settings:\n    exporter_run_period: 86400\n    encryption_key: 5t3X3PkQqF5edqW9_c8znow7k7GraZOpBLHhF1kFFvE=\n    task_wait_timeout: 259200\n  failed_imports_dataset_generator:\n    enable: false\n    bucket: \n    s3_path: \n    filename: \n    aws_access_key_id: \n    aws_secret_access_key: "
---
# Source: opticloud/templates/mariadb.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: mysql-config-vol
data:
  mariadb.cnf: |
    [mysqld]
    innodb_file_format = BARRACUDA
    innodb_large_prefix	= 1
    innodb_file_per_table = 1
    wait_timeout = 28800
    explicit_defaults_for_timestamp = 1
---
# Source: opticloud/templates/mongodb.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: "mongo-config"
  labels:
    app: "mongo"
data:
  mongod.conf: ""
---
# Source: opticloud/templates/rabbitmq.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: rabbitmq-config
  labels:
    app: rabbitmq
    release: release-name
data:
  enabled_plugins: |
    [
      rabbitmq_shovel,
      rabbitmq_shovel_management,
      rabbitmq_federation,
      rabbitmq_federation_management,
      rabbitmq_consistent_hash_exchange,
      rabbitmq_management,
      rabbitmq_peer_discovery_k8s
    ].
  rabbitmq.conf: |
    ## RabbitMQ configuration
    ## Ref: https://github.com/rabbitmq/rabbitmq-server/blob/master/docs/rabbitmq.conf.example
    
    ## Clustering
    cluster_formation.peer_discovery_backend  = rabbit_peer_discovery_k8s
    cluster_formation.k8s.host = kubernetes.default.svc.cluster.local
    cluster_formation.k8s.address_type = hostname
    cluster_formation.node_cleanup.interval = 10
    # Set to false if automatic cleanup of absent nodes is desired.
    # This can be dangerous, see http://www.rabbitmq.com/cluster-formation.html#node-health-checks-and-cleanup.
    cluster_formation.node_cleanup.only_log_warning = true
    cluster_partition_handling = autoheal
    
    management.load_definitions = /etc/definitions/definitions.json
    
    ## Memory-based Flow Control threshold
    vm_memory_high_watermark.absolute = 512MB
---
# Source: opticloud/templates/thanos_receive.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: thanos-receive-limits-config
  labels:
    app: thanos-receive
    release: release-name
data:
  thanos_limits.yaml: |
    write:
      global:
        max_concurrency: 30
---
# Source: opticloud/templates/version.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: opticloud-version
data:
  component_versions.yaml: ""
---
# Source: opticloud/templates/rabbitmq.yaml
kind: PersistentVolume
apiVersion: v1
metadata:
  name: rabbitmq-storage
  labels:
    type: local
  annotations:
    volume.alpha.kubernetes.io/storage-class: rabbitmq-storage
spec:
  storageClassName: rabbitmq-storage
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Recycle
  hostPath:
    path: "/opticloud/rabbitmq"
---
# Source: opticloud/templates/thanos_compactor.yaml
kind: PersistentVolume
apiVersion: v1
metadata:
  name: thanos-compactor
  labels:
    type: local
  annotations:
    volume.alpha.kubernetes.io/storage-class: thanos-compactor
spec:
  storageClassName: thanos-compactor
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  hostPath:
    path: "/opticloud/thanos-compactor"
---
# Source: opticloud/templates/thanos_receive.yaml
kind: PersistentVolume
apiVersion: v1
metadata:
  name: thanos-receive
  labels:
    type: local
  annotations:
    volume.alpha.kubernetes.io/storage-class: thanos-receive
spec:
  storageClassName: thanos-receive
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  hostPath:
    path: "/opticloud/thanos-receive"
---
# Source: opticloud/templates/thanos_storegateway.yaml
kind: PersistentVolume
apiVersion: v1
metadata:
  name: thanos-storegateway
  labels:
    type: local
  annotations:
    volume.alpha.kubernetes.io/storage-class: thanos-storegateway
spec:
  storageClassName: thanos-storegateway
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  hostPath:
    path: "/opticloud/thanos-storegateway"
---
# Source: opticloud/templates/rabbitmq.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: rabbitmq-claim
spec:
  storageClassName: rabbitmq-storage
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
---
# Source: opticloud/templates/thanos_compactor.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: thanos-compactor-claim
spec:
  storageClassName: thanos-compactor
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
# Source: opticloud/templates/thanos_receive.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: thanos-receive-claim
spec:
  storageClassName: thanos-receive
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
# Source: opticloud/templates/thanos_storegateway.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: thanos-storegateway-claim
spec:
  storageClassName: thanos-storegateway
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
# Source: opticloud/templates/rabbitmq.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app: rabbitmq
    release: release-name
  name: rabbitmq
rules:
- apiGroups: [""]
  resources: ["endpoints"]
  verbs: ["get"]
---
# Source: opticloud/templates/rabbitmq.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app: rabbitmq
    release: release-name
  name: rabbitmq
subjects:
- kind: ServiceAccount
  name: rabbitmq
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: rabbitmq
---
# Source: opticloud/templates/arcee.yaml
apiVersion: v1
kind: Service
metadata:
  name: arcee
  labels:
    app: arcee
    chart: opticloud-0.1.0
    release: release-name
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 8891
      protocol: TCP
      name: arcee
  selector:
    app: arcee
    release: release-name
---
# Source: opticloud/templates/auth.yaml
apiVersion: v1
kind: Service
metadata:
  name: auth
  labels:
    app: auth
    chart: opticloud-0.1.0
    release: release-name
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 8905
      protocol: TCP
      name: auth
  selector:
    app: auth
    release: release-name
---
# Source: opticloud/templates/bulldozer_api.yaml
apiVersion: v1
kind: Service
metadata:
  name: bulldozer-api
  labels:
    app: bulldozer-api
    chart: opticloud-0.1.0
    release: release-name
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 8896
      protocol: TCP
      name: bulldozer-api
  selector:
    app: bulldozer-api
    release: release-name
---
# Source: opticloud/templates/clickhouse/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: clickhouse
  labels:
    heritage: Helm
    release: release-name
    app: clickhouse
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8123
      targetPort: 8123
      protocol: TCP

    - name: native
      port: 9000
      targetPort: 9000
      protocol: TCP
  selector:
    app: clickhouse
    release: release-name
---
# Source: opticloud/templates/diproxy.yaml
apiVersion: v1
kind: Service
metadata:
  name: diproxy
  labels:
    app: diproxy
    chart: opticloud-0.1.0
    release: release-name
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 8935
      protocol: TCP
      name: diproxy
  selector:
    app: diproxy
    release: release-name
---
# Source: opticloud/templates/error_pages.yaml
apiVersion: v1
kind: Service
metadata:
  name: error-pages
  labels:
    app: error-pages
    chart: opticloud-0.1.0
    release: release-name
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 8080
      protocol: TCP
      name: error-pages
  selector:
    app: error-pages
    release: release-name
---
# Source: opticloud/templates/etcd.yaml
apiVersion: v1
kind: Service
metadata:
  name: etcd-client
  labels:
    app: etcd
    chart: opticloud-0.1.0
    release: release-name
spec:
  type: ClusterIP
  ports:
    - port: 2379
      targetPort: 2379
      protocol: TCP
      name: etcd-client
  selector:
    app: etcd
    release: release-name
---
# Source: opticloud/templates/grafana.yaml
apiVersion: v1
kind: Service
metadata:
  name: grafana
  labels:
    app: grafana
    chart: opticloud-0.1.0
    release: release-name
spec:
  type: NodePort
  ports:
    - nodePort: 30082
      port: 80
      targetPort: 80
      protocol: TCP
      name: grafana
  selector:
    app: grafana
    release: release-name
---
# Source: opticloud/templates/herald.yaml
apiVersion: v1
kind: Service
metadata:
  name: herald
  labels:
    app: herald
    chart: opticloud-0.1.0
    release: release-name
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 8906
      protocol: TCP
      name: herald
  selector:
    app: heraldapi
    release: release-name
---
# Source: opticloud/templates/influxdb.yaml
apiVersion: v1
kind: Service
metadata:
  name: influxdb
  labels:
    app: influxdb
    chart: opticloud-0.1.0
    release: release-name
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 8086
      protocol: TCP
      name: influxdb
  selector:
    app: influxdb
    release: release-name
---
# Source: opticloud/templates/insider_api.yaml
apiVersion: v1
kind: Service
metadata:
  name: insider-api
  labels:
    app: insider-api
    chart: opticloud-0.1.0
    release: release-name
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 8945
      protocol: TCP
      name: insider-api
  selector:
    app: insider-api
    release: release-name
---
# Source: opticloud/templates/jira_bus.yaml
apiVersion: v1
kind: Service
metadata:
  name: jira-bus
  labels:
    app: jira-bus
    chart: opticloud-0.1.0
    release: release-name
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 8977
      protocol: TCP
      name: jira-bus
  selector:
    app: jira-bus
    release: release-name
---
# Source: opticloud/templates/jira_ui.yaml
apiVersion: v1
kind: Service
metadata:
  name: jira-ui
  labels:
    app: jira-ui
    chart: opticloud-0.1.0
    release: release-name
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 4000
      protocol: TCP
      name: jira-ui
  selector:
    app: jira-ui
    release: release-name
---
# Source: opticloud/templates/katara.yaml
apiVersion: v1
kind: Service
metadata:
  name: katara
  labels:
    app: kataraapi
    chart: opticloud-0.1.0
    release: release-name
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 8935
      protocol: TCP
      name: katara
  selector:
    app: kataraapi
    release: release-name
---
# Source: opticloud/templates/keeper.yaml
apiVersion: v1
kind: Service
metadata:
  name: keeper
  labels:
    app: keeper
    chart: opticloud-0.1.0
    release: release-name
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 8973
      protocol: TCP
      name: keeper
  selector:
    app: keeper
    release: release-name
---
# Source: opticloud/templates/mariadb.yaml
apiVersion: v1
kind: Service
metadata:
  name: mariadb
  labels:
    app: mariadb
    chart: opticloud-0.1.0
    release: release-name
spec:
  type: ClusterIP
  ports:
    - port: 3306
      targetPort: 3306
      protocol: TCP
      name: mariadb
  selector:
    app: mariadb
    release: release-name
---
# Source: opticloud/templates/metroculus_api.yaml
apiVersion: v1
kind: Service
metadata:
  name: metroculusapi
  labels:
    app: metroculusapi
    chart: opticloud-0.1.0
    release: release-name
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 8969
      protocol: TCP
      name: metroculusapi
  selector:
    app: metroculusapi
    release: release-name
---
# Source: opticloud/templates/minio.yaml
apiVersion: v1
kind: Service
metadata:
  name: minio
  labels:
    app: minio
    chart: opticloud-0.1.0
    release: release-name
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 9000
      protocol: TCP
      name: minio
  selector:
    app: minio
    release: release-name
---
# Source: opticloud/templates/mongodb.yaml
apiVersion: v1
kind: Service
metadata:
  name: mongo
  labels:
    app: mongo
    release: release-name
spec:
  type: ClusterIP
  ports:
  - port: 80
    targetPort: 27017
    protocol: TCP
    name: mongo
  selector:
    app: mongo
    release: release-name
---
# Source: opticloud/templates/mongodb.yaml
# A headless service to create DNS records for discovery purposes
apiVersion: v1
kind: Service
metadata:
  name: "mongo-discovery"
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
  labels:
    app: mongo
    release: release-name
spec:
  type: ClusterIP
  clusterIP: None
  ports:
  - name: mongodb
    port: 27017
  publishNotReadyAddresses: true
  selector:
    app: mongo
    release: release-name
---
# Source: opticloud/templates/myadmin.yaml
apiVersion: v1
kind: Service
metadata:
  name: myadmin
  labels:
    app: myadmin
    chart: opticloud-0.1.0
    release: release-name
spec:
  type: NodePort
  ports:
    - nodePort: 30080
      port: 80
      targetPort: 80
      protocol: TCP
      name: myadmin
  selector:
    app: myadmin
    release: release-name
---
# Source: opticloud/templates/ngui.yaml
apiVersion: v1
kind: Service
metadata:
  name: ngui
  labels:
    app: ngui
    chart: opticloud-0.1.0
    release: release-name
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 4000
      protocol: TCP
      name: ngui
  selector:
    app: ngui
    release: release-name
---
# Source: opticloud/templates/ohsu.yaml
apiVersion: v1
kind: Service
metadata:
  name: ohsu
  labels:
    app: ohsu
    chart: opticloud-0.1.0
    release: release-name
spec:
  type: NodePort
  ports:
    - port: 30180
      targetPort: 30180
      containerPort: 30180
      nodePort: 30180
      name: shs-port-30180
    - port: 30181
      targetPort: 30181
      containerPort: 30181
      nodePort: 30181
      name: shs-port-30181
    - port: 30182
      targetPort: 30182
      containerPort: 30182
      nodePort: 30182
      name: shs-port-30182
    - port: 30183
      targetPort: 30183
      containerPort: 30183
      nodePort: 30183
      name: shs-port-30183
    - port: 30184
      targetPort: 30184
      containerPort: 30184
      nodePort: 30184
      name: shs-port-30184
    - port: 30185
      targetPort: 30185
      containerPort: 30185
      nodePort: 30185
      name: shs-port-30185
    - port: 30186
      targetPort: 30186
      containerPort: 30186
      nodePort: 30186
      name: shs-port-30186
    - port: 30187
      targetPort: 30187
      containerPort: 30187
      nodePort: 30187
      name: shs-port-30187
    - port: 30188
      targetPort: 30188
      containerPort: 30188
      nodePort: 30188
      name: shs-port-30188
    - port: 30189
      targetPort: 30189
      containerPort: 30189
      nodePort: 30189
      name: shs-port-30189
    - port: 30190
      targetPort: 30190
      containerPort: 30190
      nodePort: 30190
      name: shs-port-30190
    - port: 30191
      targetPort: 30191
      containerPort: 30191
      nodePort: 30191
      name: shs-port-30191
    - port: 30192
      targetPort: 30192
      containerPort: 30192
      nodePort: 30192
      name: shs-port-30192
    - port: 30193
      targetPort: 30193
      containerPort: 30193
      nodePort: 30193
      name: shs-port-30193
    - port: 30194
      targetPort: 30194
      containerPort: 30194
      nodePort: 30194
      name: shs-port-30194
    - port: 30195
      targetPort: 30195
      containerPort: 30195
      nodePort: 30195
      name: shs-port-30195
    - port: 30196
      targetPort: 30196
      containerPort: 30196
      nodePort: 30196
      name: shs-port-30196
    - port: 30197
      targetPort: 30197
      containerPort: 30197
      nodePort: 30197
      name: shs-port-30197
    - port: 30198
      targetPort: 30198
      containerPort: 30198
      nodePort: 30198
      name: shs-port-30198
    - port: 30199
      targetPort: 30199
      containerPort: 30199
      nodePort: 30199
      name: shs-port-30199
    - port: 30200
      targetPort: 30200
      containerPort: 30200
      nodePort: 30200
      name: shs-port-30200
    - port: 30201
      targetPort: 30201
      containerPort: 30201
      nodePort: 30201
      name: shs-port-30201
    - port: 30202
      targetPort: 30202
      containerPort: 30202
      nodePort: 30202
      name: shs-port-30202
    - port: 30203
      targetPort: 30203
      containerPort: 30203
      nodePort: 30203
      name: shs-port-30203
    - port: 30204
      targetPort: 30204
      containerPort: 30204
      nodePort: 30204
      name: shs-port-30204
    - port: 30205
      targetPort: 30205
      containerPort: 30205
      nodePort: 30205
      name: shs-port-30205
    - port: 30206
      targetPort: 30206
      containerPort: 30206
      nodePort: 30206
      name: shs-port-30206
    - port: 30207
      targetPort: 30207
      containerPort: 30207
      nodePort: 30207
      name: shs-port-30207
    - port: 30208
      targetPort: 30208
      containerPort: 30208
      nodePort: 30208
      name: shs-port-30208
    - port: 30209
      targetPort: 30209
      containerPort: 30209
      nodePort: 30209
      name: shs-port-30209
    - port: 30210
      targetPort: 30210
      containerPort: 30210
      nodePort: 30210
      name: shs-port-30210
    - port: 30211
      targetPort: 30211
      containerPort: 30211
      nodePort: 30211
      name: shs-port-30211
    - port: 30212
      targetPort: 30212
      containerPort: 30212
      nodePort: 30212
      name: shs-port-30212
    - port: 30213
      targetPort: 30213
      containerPort: 30213
      nodePort: 30213
      name: shs-port-30213
    - port: 30214
      targetPort: 30214
      containerPort: 30214
      nodePort: 30214
      name: shs-port-30214
    - port: 30215
      targetPort: 30215
      containerPort: 30215
      nodePort: 30215
      name: shs-port-30215
    - port: 30216
      targetPort: 30216
      containerPort: 30216
      nodePort: 30216
      name: shs-port-30216
    - port: 30217
      targetPort: 30217
      containerPort: 30217
      nodePort: 30217
      name: shs-port-30217
    - port: 30218
      targetPort: 30218
      containerPort: 30218
      nodePort: 30218
      name: shs-port-30218
    - port: 30219
      targetPort: 30219
      containerPort: 30219
      nodePort: 30219
      name: shs-port-30219
    - port: 30220
      targetPort: 30220
      containerPort: 30220
      nodePort: 30220
      name: shs-port-30220
    - port: 30221
      targetPort: 30221
      containerPort: 30221
      nodePort: 30221
      name: shs-port-30221
    - port: 30222
      targetPort: 30222
      containerPort: 30222
      nodePort: 30222
      name: shs-port-30222
    - port: 30223
      targetPort: 30223
      containerPort: 30223
      nodePort: 30223
      name: shs-port-30223
    - port: 30224
      targetPort: 30224
      containerPort: 30224
      nodePort: 30224
      name: shs-port-30224
    - port: 30225
      targetPort: 30225
      containerPort: 30225
      nodePort: 30225
      name: shs-port-30225
    - port: 30226
      targetPort: 30226
      containerPort: 30226
      nodePort: 30226
      name: shs-port-30226
    - port: 30227
      targetPort: 30227
      containerPort: 30227
      nodePort: 30227
      name: shs-port-30227
    - port: 30228
      targetPort: 30228
      containerPort: 30228
      nodePort: 30228
      name: shs-port-30228
    - port: 30229
      targetPort: 30229
      containerPort: 30229
      nodePort: 30229
      name: shs-port-30229
    - port: 30230
      targetPort: 30230
      containerPort: 30230
      nodePort: 30230
      name: shs-port-30230
    - port: 30231
      targetPort: 30231
      containerPort: 30231
      nodePort: 30231
      name: shs-port-30231
    - port: 30232
      targetPort: 30232
      containerPort: 30232
      nodePort: 30232
      name: shs-port-30232
    - port: 30233
      targetPort: 30233
      containerPort: 30233
      nodePort: 30233
      name: shs-port-30233
    - port: 30234
      targetPort: 30234
      containerPort: 30234
      nodePort: 30234
      name: shs-port-30234
    - port: 30235
      targetPort: 30235
      containerPort: 30235
      nodePort: 30235
      name: shs-port-30235
    - port: 30236
      targetPort: 30236
      containerPort: 30236
      nodePort: 30236
      name: shs-port-30236
    - port: 30237
      targetPort: 30237
      containerPort: 30237
      nodePort: 30237
      name: shs-port-30237
    - port: 30238
      targetPort: 30238
      containerPort: 30238
      nodePort: 30238
      name: shs-port-30238
    - port: 30239
      targetPort: 30239
      containerPort: 30239
      nodePort: 30239
      name: shs-port-30239
    - port: 30240
      targetPort: 30240
      containerPort: 30240
      nodePort: 30240
      name: shs-port-30240
    - port: 30241
      targetPort: 30241
      containerPort: 30241
      nodePort: 30241
      name: shs-port-30241
    - port: 30242
      targetPort: 30242
      containerPort: 30242
      nodePort: 30242
      name: shs-port-30242
    - port: 30243
      targetPort: 30243
      containerPort: 30243
      nodePort: 30243
      name: shs-port-30243
    - port: 30244
      targetPort: 30244
      containerPort: 30244
      nodePort: 30244
      name: shs-port-30244
    - port: 30245
      targetPort: 30245
      containerPort: 30245
      nodePort: 30245
      name: shs-port-30245
    - port: 30246
      targetPort: 30246
      containerPort: 30246
      nodePort: 30246
      name: shs-port-30246
    - port: 30247
      targetPort: 30247
      containerPort: 30247
      nodePort: 30247
      name: shs-port-30247
    - port: 30248
      targetPort: 30248
      containerPort: 30248
      nodePort: 30248
      name: shs-port-30248
    - port: 30249
      targetPort: 30249
      containerPort: 30249
      nodePort: 30249
      name: shs-port-30249
    - port: 30250
      targetPort: 30250
      containerPort: 30250
      nodePort: 30250
      name: shs-port-30250
    - port: 30251
      targetPort: 30251
      containerPort: 30251
      nodePort: 30251
      name: shs-port-30251
    - port: 30252
      targetPort: 30252
      containerPort: 30252
      nodePort: 30252
      name: shs-port-30252
    - port: 30253
      targetPort: 30253
      containerPort: 30253
      nodePort: 30253
      name: shs-port-30253
    - port: 30254
      targetPort: 30254
      containerPort: 30254
      nodePort: 30254
      name: shs-port-30254
    - port: 30255
      targetPort: 30255
      containerPort: 30255
      nodePort: 30255
      name: shs-port-30255
    - port: 30256
      targetPort: 30256
      containerPort: 30256
      nodePort: 30256
      name: shs-port-30256
    - port: 30257
      targetPort: 30257
      containerPort: 30257
      nodePort: 30257
      name: shs-port-30257
    - port: 30258
      targetPort: 30258
      containerPort: 30258
      nodePort: 30258
      name: shs-port-30258
    - port: 30259
      targetPort: 30259
      containerPort: 30259
      nodePort: 30259
      name: shs-port-30259
    - port: 30260
      targetPort: 30260
      containerPort: 30260
      nodePort: 30260
      name: shs-port-30260
    - port: 30261
      targetPort: 30261
      containerPort: 30261
      nodePort: 30261
      name: shs-port-30261
    - port: 30262
      targetPort: 30262
      containerPort: 30262
      nodePort: 30262
      name: shs-port-30262
    - port: 30263
      targetPort: 30263
      containerPort: 30263
      nodePort: 30263
      name: shs-port-30263
    - port: 30264
      targetPort: 30264
      containerPort: 30264
      nodePort: 30264
      name: shs-port-30264
    - port: 30265
      targetPort: 30265
      containerPort: 30265
      nodePort: 30265
      name: shs-port-30265
    - port: 30266
      targetPort: 30266
      containerPort: 30266
      nodePort: 30266
      name: shs-port-30266
    - port: 30267
      targetPort: 30267
      containerPort: 30267
      nodePort: 30267
      name: shs-port-30267
    - port: 30268
      targetPort: 30268
      containerPort: 30268
      nodePort: 30268
      name: shs-port-30268
    - port: 30269
      targetPort: 30269
      containerPort: 30269
      nodePort: 30269
      name: shs-port-30269
    - port: 30270
      targetPort: 30270
      containerPort: 30270
      nodePort: 30270
      name: shs-port-30270
    - port: 30271
      targetPort: 30271
      containerPort: 30271
      nodePort: 30271
      name: shs-port-30271
    - port: 30272
      targetPort: 30272
      containerPort: 30272
      nodePort: 30272
      name: shs-port-30272
    - port: 30273
      targetPort: 30273
      containerPort: 30273
      nodePort: 30273
      name: shs-port-30273
    - port: 30274
      targetPort: 30274
      containerPort: 30274
      nodePort: 30274
      name: shs-port-30274
    - port: 30275
      targetPort: 30275
      containerPort: 30275
      nodePort: 30275
      name: shs-port-30275
    - port: 30276
      targetPort: 30276
      containerPort: 30276
      nodePort: 30276
      name: shs-port-30276
    - port: 30277
      targetPort: 30277
      containerPort: 30277
      nodePort: 30277
      name: shs-port-30277
    - port: 30278
      targetPort: 30278
      containerPort: 30278
      nodePort: 30278
      name: shs-port-30278
    - port: 30279
      targetPort: 30279
      containerPort: 30279
      nodePort: 30279
      name: shs-port-30279
    - port: 80
      targetPort: 9377
      protocol: TCP
      name: ohsu
  selector:
    app: ohsu
    release: release-name
---
# Source: opticloud/templates/pharos_receiver.yaml
apiVersion: v1
kind: Service
metadata:
  name: pharos-receiver
  labels:
    app: pharos-receiver
    chart: opticloud-0.1.0
    release: release-name
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 8946
      protocol: TCP
      name: pharos-receiver
  selector:
    app: pharos-receiver
    release: release-name
---
# Source: opticloud/templates/rabbitmq.yaml
# discovery service
apiVersion: v1
kind: Service
metadata:
  name: rabbitmq-discovery
  labels:
    app: rabbitmq
    release: "release-name"
spec:
  clusterIP: None
  ports:
    - name: http
      protocol: TCP
      port: 15672
      targetPort: http
    - name: amqp
      protocol: TCP
      port: 5672
      targetPort: amqp
    - name: epmd
      protocol: TCP
      port: 4369
      targetPort: epmd
  publishNotReadyAddresses: true
  selector:
    app: rabbitmq
    release: release-name
  type: ClusterIP
---
# Source: opticloud/templates/rabbitmq.yaml
apiVersion: v1
kind: Service
metadata:
  name: rabbitmq
  labels:
    app: rabbitmq
    chart: opticloud-0.1.0
    release: release-name
spec:
  type: ClusterIP
  ports:
    - port: 5672
      targetPort: 5672
      protocol: TCP
      name: rabbitmq
    - name: http
      protocol: TCP
      port: 15672
      targetPort: http
    - name: epmd
      protocol: TCP
      port: 4369
      targetPort: epmd
  selector:
    app: rabbitmq
    release: release-name
---
# Source: opticloud/templates/redis.yaml
apiVersion: v1
kind: Service
metadata:
  name: redis
  labels:
    app: redis
    chart: opticloud-0.1.0
    release: release-name
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 6379
      protocol: TCP
      name: redis
  selector:
    app: redis
    release: release-name
---
# Source: opticloud/templates/restapi.yaml
apiVersion: v1
kind: Service
metadata:
  name: restapi
  labels:
    app: restapi
    chart: opticloud-0.1.0
    release: release-name
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 8999
      protocol: TCP
      name: restapi
  selector:
    app: restapi
    release: release-name
---
# Source: opticloud/templates/slacker.yaml
apiVersion: v1
kind: Service
metadata:
  name: slacker
  labels:
    app: slacker
    chart: opticloud-0.1.0
    release: release-name
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: slacker
  selector:
    app: slacker
    release: release-name
---
# Source: opticloud/templates/thanos_compactor.yaml
apiVersion: v1
kind: Service
metadata:
  name: thanos-compactor
  labels:
    app: thanos-compactor
    release: release-name
spec:
  type: ClusterIP
  ports:
    - port: 10902
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app: thanos-compactor
    release: release-name
---
# Source: opticloud/templates/thanos_query.yaml
apiVersion: v1
kind: Service
metadata:
  name: thanos-query
  labels:
    app: thanos-query
    release: release-name
spec:
  type: ClusterIP
  ports:
    - port: 10902
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app: thanos-query
    release: release-name
---
# Source: opticloud/templates/thanos_query.yaml
apiVersion: v1
kind: Service
metadata:
  name: thanos-query-grpc
  labels:
    app: thanos-query
    release: release-name
spec:
  type: ClusterIP
  ports:
    - port: 10901
      targetPort: grpc
      protocol: TCP
      name: grpc
  selector:
    app: thanos-query
    release: release-name
---
# Source: opticloud/templates/thanos_receive.yaml
apiVersion: v1
kind: Service
metadata:
  name: thanos-receive
  labels:
    app: thanos-receive
    release: release-name
spec:
  type: ClusterIP
  ports:
    - port: 10902
      targetPort: http
      protocol: TCP
      name: http
    - port: 10901
      targetPort: grpc
      protocol: TCP
      name: grpc
    - port: 19291
      targetPort: remote
      protocol: TCP
      name: remote
  selector:
    app: thanos-receive
    release: release-name
---
# Source: opticloud/templates/thanos_storegateway.yaml
apiVersion: v1
kind: Service
metadata:
  name: thanos-storegateway
  labels:
    app: thanos-storegateway
    release: release-name
spec:
  type: ClusterIP
  ports:
    - port: 10902
      targetPort: http
      protocol: TCP
      name: http
    - port: 10901
      targetPort: grpc
      protocol: TCP
      name: grpc
  selector:
    app: thanos-storegateway
    release: release-name
---
# Source: opticloud/templates/thanos_web.yaml
apiVersion: v1
kind: Service
metadata:
  name: thanos-web
  labels:
    app: thanos-web
    release: release-name
spec:
  type: ClusterIP
  ports:
    - port: 10902
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app: thanos-web
    release: release-name
---
# Source: opticloud/templates/arcee.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: arcee
  labels:
    app: arcee
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: arcee
      release: release-name
  serviceName: arcee
  template:
    metadata:
      labels:
        app: arcee
        release: release-name
    spec:
      initContainers:
      - name: "wait-mongo"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z mongo.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      containers:
      - name: arcee
        image: "arcee:local"
        imagePullPolicy: Never
        ports:
        - containerPort: 8891
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
        readinessProbe:
          tcpSocket:
            port: 8891
            initialDelaySeconds: 5
            periodSeconds: 10
---
# Source: opticloud/templates/auth.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: auth
  labels:
    app: auth
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: auth
      release: release-name
  serviceName: auth
  template:
    metadata:
      labels:
        app: auth
        release: release-name
    spec:
      initContainers:
      - name: "wait-etcd-client"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z etcd-client.default.svc.cluster.local 2379 -w 2; do sleep 2; done']
      - name: wait-mariadb
        image: "mariadb:local"
        imagePullPolicy: Never
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mariadb-secret
              key: password
        command: ['sh', '-c', 'until mysql --connect-timeout=2 -h mariadb.default.svc.cluster.local -p$MYSQL_ROOT_PASSWORD -e "SELECT 1"; do sleep 2; done']
      containers:
      - name: auth
        image: "auth:local"
        imagePullPolicy: Never
        ports:
        - containerPort: 8905
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
        - name: GOOGLE_OAUTH_CLIENT_ID
          value: 
        - name: GOOGLE_OAUTH_CLIENT_SECRET
          value: 
        - name: MICROSOFT_OAUTH_CLIENT_ID
          value: 
        readinessProbe:
          tcpSocket:
            port: 8905
            initialDelaySeconds: 5
            periodSeconds: 10
---
# Source: opticloud/templates/bi_exporter.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: bi-exporter
  labels:
    app: bi-exporter
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: bi-exporter
      release: release-name
  template:
    metadata:
      labels:
        app: bi-exporter
        release: release-name
    spec:
      initContainers:
      - name: "wait-rabbitmq"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
      - name: "wait-mongo"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z mongo.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      - name: "wait-restapi"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      - name: "wait-clickhouse"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z clickhouse.default.svc.cluster.local 9000 -w 2; do sleep 2; done']
      containers:
      - name: bi-exporter
        image: "bi_exporter:local"
        imagePullPolicy: Never
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
        volumeMounts:
          - name: bi-exporter-data-volume
            mountPath: /usr/src/app/bi_exporter/bi
      volumes:
        - name: bi-exporter-data-volume
          hostPath:
            path: /opticloud/bi
            type: DirectoryOrCreate
---
# Source: opticloud/templates/booking_observer.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: booking-observer-worker
  labels:
    app: booking-observer-worker
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 2
  selector:
    matchLabels:
      app: booking-observer-worker
      release: release-name
  template:
    metadata:
      labels:
        app: booking-observer-worker
        release: release-name
    spec:
      initContainers:
      - name: "wait-rabbitmq"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
      - name: "wait-restapi"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      - name: "wait-mongo"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z mongo.default.svc.cluster.local 80 -w 2; do sleep 2; done']

      containers:
      - name: booking-observer-worker
        image: "booking_observer:local"
        imagePullPolicy: Never
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
---
# Source: opticloud/templates/bulldozer_api.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: bulldozer-api
  labels:
    app: bulldozer-api
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: bulldozer-api
      release: release-name
  serviceName: bulldozer-api
  template:
    metadata:
      labels:
        app: bulldozer-api
        release: release-name
    spec:
      initContainers:
      - name: "wait-mongo"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z mongo.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      containers:
      - name: bulldozer-api
        image: "bulldozer_api:local"
        imagePullPolicy: Never
        ports:
        - containerPort: 8896
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
        readinessProbe:
          tcpSocket:
            port: 8896
            initialDelaySeconds: 5
            periodSeconds: 10
---
# Source: opticloud/templates/bulldozer_worker.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: bulldozerworker
  labels:
    app: bulldozerworker
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: bulldozerworker
      release: release-name
  template:
    metadata:
      labels:
        app: bulldozerworker
        release: release-name
    spec:
      initContainers:
      - name: "wait-rabbitmq"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
      - name: "wait-restapi"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      - name: "wait-bulldozer-api"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z bulldozer-api.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      - name: "wait-arcee"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z arcee.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      containers:
      - name: bulldozerworker
        image: "bulldozer_worker:local"
        imagePullPolicy: Never
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
        - name: FAKE_CAD_ENABLED
          value: "0"
        volumeMounts:
          - name: bulldozer-data-volume
            mountPath: /var/lib/bulldozer/
      volumes:
        - name: bulldozer-data-volume
          hostPath:
            path: /opticloud/bulldozer
            type: DirectoryOrCreate
---
# Source: opticloud/templates/bumi_scheduler.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: bumischeduler
  labels:
    app: bumischeduler
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: bumischeduler
      release: release-name
  template:
    metadata:
      labels:
        app: bumischeduler
        release: release-name
    spec:
      initContainers:
      - name: "wait-etcd-client"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z etcd-client.default.svc.cluster.local 2379 -w 2; do sleep 2; done']
      - name: "wait-rabbitmq"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
      containers:
      - name: bumischeduler
        image: "bumischeduler:local"
        imagePullPolicy: Never
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
---
# Source: opticloud/templates/bumi_worker.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: bumiworker
  labels:
    app: bumiworker
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: bumiworker
      release: release-name
  template:
    metadata:
      labels:
        app: bumiworker
        release: release-name
    spec:
      initContainers:
      - name: "wait-rabbitmq"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
      - name: "wait-restapi"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      containers:
      - name: bumiworker
        image: "bumiworker:local"
        imagePullPolicy: Never
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
        - name: FAKE_CAD_ENABLED
          value: "0"
---
# Source: opticloud/templates/calendar_observer.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: calendar-observer-worker
  labels:
    app: calendar-observer-worker
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: calendar-observer-worker
      release: release-name
  template:
    metadata:
      labels:
        app: calendar-observer-worker
        release: release-name
    spec:
      initContainers:
      - name: "wait-rabbitmq"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
      - name: "wait-restapi"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      containers:
      - name: calendar-observer-worker
        image: "calendar_observer:local"
        imagePullPolicy: Never
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
---
# Source: opticloud/templates/diproxy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: diproxy
  labels:
    app: diproxy
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: diproxy
      release: release-name
  serviceName: diproxy
  template:
    metadata:
      labels:
        app: diproxy
        release: release-name
    spec:
      initContainers:
      - name: "wait-restapi"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      - name: "wait-thanos-receive"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z thanos-receive.default.svc.cluster.local 10902 -w 2; do sleep 2; done']
      containers:
      - name: diproxy
        image: "diproxy:local"
        imagePullPolicy: Never
        ports:
        - containerPort: 8935
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
        readinessProbe:
          tcpSocket:
            port: 8935
            initialDelaySeconds: 5
            periodSeconds: 10
---
# Source: opticloud/templates/diworker.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: diworker
  labels:
    app: diworker
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: diworker
      release: release-name
  template:
    metadata:
      labels:
        app: diworker
        release: release-name
    spec:
      initContainers:
      - name: "wait-rabbitmq"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
      - name: "wait-mongo"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z mongo.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      - name: "wait-clickhouse"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z clickhouse.default.svc.cluster.local 9000 -w 2; do sleep 2; done']
      - name: "wait-restapi"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      containers:
      - name: diworker
        image: "diworker:local"
        imagePullPolicy: Never
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
        - name: FAKE_CAD_ENABLED
          value: "0"
---
# Source: opticloud/templates/error_pages.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: error-pages
  labels:
    app: error-pages
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: error-pages
      release: release-name
  serviceName: error-pages
  template:
    metadata:
      labels:
        app: error-pages
        release: release-name
    spec:
      containers:
      - name: error-pages
        image: "error_pages:local"
        imagePullPolicy: Never
        ports:
        - containerPort: 8080
---
# Source: opticloud/templates/gemini_worker.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gemini-worker
  labels:
    app: gemini-worker
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: gemini-worker
      release: release-name
  template:
    metadata:
      labels:
        app: gemini-worker
        release: release-name
    spec:
      initContainers:
      - name: "wait-rabbitmq"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
      - name: "wait-etcd-client"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z etcd-client.default.svc.cluster.local 2379 -w 2; do sleep 2; done']
      - name: "wait-restapi"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      containers:
      - name: gemini-worker
        image: "gemini_worker:local"
        imagePullPolicy: Never
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
        volumeMounts:
          - name: gemini-worker-data-volume
            mountPath: /usr/src/app/gemini_worker/cache
      volumes:
        - name: gemini-worker-data-volume
          hostPath:
            path: /opticloud/gemini_worker
            type: DirectoryOrCreate
---
# Source: opticloud/templates/grafana.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  labels:
    app: grafana
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
      release: release-name
  serviceName: grafana
  template:
    metadata:
      labels:
        app: grafana
        release: release-name
    spec:
      initContainers:
      - name: "wait-influxdb"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z influxdb.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      containers:
      - name: grafana
        image: "grafana:local"
        imagePullPolicy: Never
        ports:
        - containerPort: 3000
        env:
        - name: IMAGE_ID
          value: 
        - name: GF_USERS_ALLOW_SIGN_UP
          value: "false"
        - name: GF_USERS_AUTO_ASSIGN_ORG
          value: "true"
        - name: GF_USERS_AUTO_ASSIGN_ORG_ROLE
          value: "Editor"
        - name: GF_AUTH_PROXY_ENABLED
          value: "true"
        - name: GF_AUTH_PROXY_HEADER_NAME
          value: "X-WEBAUTH-USER"
        - name: GF_AUTH_PROXY_HEADER_PROPERTY
          value: "username"
        - name: GF_AUTH_PROXY_AUTO_SIGN_UP
          value: "true"
        - name: GF_AUTH_DISABLE_LOGIN_FORM
          value: "true"
        - name: GF_AUTH_DISABLE_SIGNOUT_MENU
          value: "true"
        - name: GF_AUTH_BASIC_ENABLED
          value: "false"
      - name: "grafana-nginx"
        image: "grafana_nginx:local"
        imagePullPolicy: Never
        ports:
        - containerPort: 80
        env:
          - name: HTPASSWD_USER
            value: userforgrafana
          - name: HTPASSWD_PASS
            value: passwordforgrafana
        readinessProbe:
          tcpSocket:
            port: 80
            initialDelaySeconds: 5
            periodSeconds: 10
      volumes:
      - name: shared
        emptyDir: {}
---
# Source: opticloud/templates/herald.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: heraldapi
  labels:
    app: heraldapi
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: heraldapi
      release: release-name
  serviceName: herald
  template:
    metadata:
      labels:
        app: heraldapi
        release: release-name
    spec:
      initContainers:
      - name: "wait-rabbitmq"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
      containers:
      - name: heraldapi
        image: "herald:local"
        imagePullPolicy: Never
        ports:
        - containerPort: 8906
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
        - name: HERALD_SERVICE
          value: api
        readinessProbe:
          tcpSocket:
            port: 8906
            initialDelaySeconds: 5
            periodSeconds: 10
---
# Source: opticloud/templates/herald.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: heraldengine
  labels:
    app: heraldengine
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 2
  selector:
    matchLabels:
      app: heraldengine
      release: release-name
  template:
    metadata:
      labels:
        app: heraldengine
        release: release-name
    spec:
      initContainers:
      - name: "wait-rabbitmq"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
      containers:
      - name: heraldengine
        image: "herald:local"
        imagePullPolicy: Never
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
        - name: HERALD_SERVICE
          value: engine
---
# Source: opticloud/templates/herald_executor.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: herald-executor
  labels:
    app: herald-executor
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: herald-executor
      release: release-name
  template:
    metadata:
      labels:
        app: herald-executor
        release: release-name
    spec:
      initContainers:
      - name: "wait-rabbitmq"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
      containers:
      - name: herald-executor
        image: "herald_executor:local"
        imagePullPolicy: Never
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
---
# Source: opticloud/templates/insider_api.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: insider-api
  labels:
    app: insider-api
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: insider-api
      release: release-name
  serviceName: insider-api
  template:
    metadata:
      labels:
        app: insider-api
        release: release-name
    spec:
      initContainers:
      - name: "wait-etcd-client"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z etcd-client.default.svc.cluster.local 2379 -w 2; do sleep 2; done']
      - name: "wait-mongo"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z mongo.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      containers:
      - name: insider-api
        image: "insider_api:local"
        imagePullPolicy: Never
        ports:
        - containerPort: 8945
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
---
# Source: opticloud/templates/insider_worker.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: insider-worker
  labels:
    app: insider-worker
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: insider-worker
      release: release-name
  template:
    metadata:
      labels:
        app: insider-worker
        release: release-name
    spec:
      initContainers:
      - name: "wait-rabbitmq"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
      - name: "wait-etcd-client"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z etcd-client.default.svc.cluster.local 2379 -w 2; do sleep 2; done']
      - name: "wait-mongo"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z mongo.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      containers:
      - name: insider-worker
        image: "insider_worker:local"
        imagePullPolicy: Never
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
---
# Source: opticloud/templates/jira_bus.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jira-bus
  labels:
    app: jira-bus
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  strategy:
    type: Recreate
    rollingUpdate: null
  selector:
    matchLabels:
      app: jira-bus
      release: release-name
  serviceName: jira-bus
  template:
    metadata:
      labels:
        app: jira-bus
        release: release-name
    spec:
      initContainers:
      - name: "wait-mariadb"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z mariadb.default.svc.cluster.local 3306 -w 2; do sleep 2; done']
      - name: "wait-auth"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z auth.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      - name: "wait-restapi"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      containers:
      - name: jira-bus
        image: "jira_bus:local"
        imagePullPolicy: Never
        ports:
        - containerPort: 8977
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
        readinessProbe:
          tcpSocket:
            port: 8977
            initialDelaySeconds: 5
            periodSeconds: 10
---
# Source: opticloud/templates/jira_ui.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jira-ui
  labels:
    app: jira-ui
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  strategy:
    type: Recreate
    rollingUpdate: null
  selector:
    matchLabels:
      app: jira-ui
      release: release-name
  serviceName: jira-ui
  template:
    metadata:
      labels:
        app: jira-ui
        release: release-name
    spec:
      initContainers:
      - name: "wait-jira-bus"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z jira-bus.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      containers:
      - name: jira-ui
        image: "jira_ui:local"
        imagePullPolicy: Never
        ports:
        - containerPort: 4000
        env:
        - name: IMAGE_ID
          value: 
        - name: UI_BUILD_PATH
          value: /usr/src/app/ui
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
        readinessProbe:
          tcpSocket:
            port: 4000
            initialDelaySeconds: 5
            periodSeconds: 10
---
# Source: opticloud/templates/katara.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: katarascheduler
  labels:
    app: katarascheduler
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: katarascheduler
      release: release-name
  template:
    metadata:
      labels:
        app: katarascheduler
        release: release-name
    spec:
      initContainers:
      - name: "wait-katara"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z katara.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      containers:
      - name: katarascheduler
        image: "katara_service:local"
        imagePullPolicy: Never
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
        - name: HX_KATARA_ROLE
          value: scheduler
---
# Source: opticloud/templates/katara.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kataraapi
  labels:
    app: kataraapi
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kataraapi
      release: release-name
  serviceName: katara
  template:
    metadata:
      labels:
        app: kataraapi
        release: release-name
    spec:
      initContainers:
      - name: "wait-etcd-client"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z etcd-client.default.svc.cluster.local 2379 -w 2; do sleep 2; done']
      - name: "wait-rabbitmq"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
      - name: "wait-restapi"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      containers:
      - name: kataraapi
        image: "katara_service:local"
        imagePullPolicy: Never
        ports:
        - containerPort: 8935
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
        - name: HX_KATARA_ROLE
          value: api
---
# Source: opticloud/templates/kataraworker.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kataraworker
  labels:
    app: kataraworker
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kataraworker
      release: release-name
  template:
    metadata:
      labels:
        app: kataraworker
        release: release-name
    spec:
      initContainers:
      - name: "wait-katara"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z katara.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      containers:
      - name: kataraworker
        image: "katara_worker:local"
        imagePullPolicy: Never
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
---
# Source: opticloud/templates/keeper.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: keeper
  labels:
    app: keeper
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: keeper
      release: release-name
  serviceName: keeper
  template:
    metadata:
      labels:
        app: keeper
        release: release-name
    spec:
      initContainers:
      - name: "wait-mongo"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z mongo.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      containers:
      - name: keeper
        image: "keeper:local"
        imagePullPolicy: Never
        ports:
        - containerPort: 8973
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
        readinessProbe:
          tcpSocket:
            port: 8973
            initialDelaySeconds: 5
            periodSeconds: 10
---
# Source: opticloud/templates/keeper_executor.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: keeper-executor
  labels:
    app: keeper-executor
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: keeper-executor
      release: release-name
  template:
    metadata:
      labels:
        app: keeper-executor
        release: release-name
    spec:
      initContainers:
      - name: "wait-rabbitmq"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
      containers:
      - name: keeper-executor
        image: "keeper_executor:local"
        imagePullPolicy: Never
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
---
# Source: opticloud/templates/live_demo_generator.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: live-demo-generator-worker
  labels:
    app: live-demo-generator-worker
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: live-demo-generator-worker
      release: release-name
  template:
    metadata:
      labels:
        app: live-demo-generator-worker
        release: release-name
    spec:
      initContainers:
      - name: "wait-rabbitmq"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
      - name: "wait-restapi"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      - name: "wait-mongo"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z mongo.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      containers:
      - name: live-demo-generator-worker
        image: "live_demo_generator:local"
        imagePullPolicy: Never
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
---
# Source: opticloud/templates/metroculus_api.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: metroculusapi
  labels:
    app: metroculusapi
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: metroculusapi
      release: release-name
  serviceName: metroculusapi
  template:
    metadata:
      labels:
        app: metroculusapi
        release: release-name
    spec:
      initContainers:
      - name: "wait-etcd-client"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z etcd-client.default.svc.cluster.local 2379 -w 2; do sleep 2; done']
      - name: "wait-clickhouse"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z clickhouse.default.svc.cluster.local 9000 -w 2; do sleep 2; done']
      containers:
      - name: metroculusapi
        image: "metroculus_api:local"
        imagePullPolicy: Never
        ports:
        - containerPort: 8969
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
---
# Source: opticloud/templates/metroculus_worker.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: metroculusworker
  labels:
    app: metroculusworker
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: metroculusworker
      release: release-name
  template:
    metadata:
      labels:
        app: metroculusworker
        release: release-name
    spec:
      initContainers:
      - name: "wait-rabbitmq"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
      - name: "wait-etcd-client"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z etcd-client.default.svc.cluster.local 2379 -w 2; do sleep 2; done']
      - name: "wait-mongo"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z mongo.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      - name: "wait-restapi"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      - name: "wait-clickhouse"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z clickhouse.default.svc.cluster.local 9000 -w 2; do sleep 2; done']
      containers:
      - name: metroculusworker
        image: "metroculus_worker:local"
        imagePullPolicy: Never
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
---
# Source: opticloud/templates/myadmin.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myadmin
  labels:
    app: myadmin
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myadmin
      release: release-name
  serviceName: myadmin
  template:
    metadata:
      labels:
        app: myadmin
        release: release-name
    spec:
      initContainers:
      - name: wait-mariadb
        image: "mariadb:local"
        imagePullPolicy: Never
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mariadb-secret
              key: password
        command: ['sh', '-c', 'until mysql --connect-timeout=2 -h mariadb.default.svc.cluster.local -p$MYSQL_ROOT_PASSWORD -e "SELECT 1"; do sleep 2; done']
      containers:
      - name: myadmin
        image: "phpmyadmin/phpmyadmin:4.7"
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
        env:
        - name: IMAGE_ID
          value: 
        - name: PMA_HOST
          value: mariadb
---
# Source: opticloud/templates/ngui.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ngui
  labels:
    app: ngui
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ngui
      release: release-name
  serviceName: ngui
  template:
    metadata:
      labels:
        app: ngui
        release: release-name
    spec:
      initContainers:
      - name: "wait-auth"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z auth.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      - name: "wait-jira-bus"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z jira-bus.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      - name: "wait-restapi"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      - name: "wait-keeper"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z keeper.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      - name: "wait-slacker"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z slacker.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      - name: "wait-redis"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z redis.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      containers:
      - name: ngui
        image: "ngui:local"
        imagePullPolicy: Never
        env:
        - name: IMAGE_ID
          value: 
        - name: BUILD_MODE
          value: 
        - name: UI_BUILD_PATH
          value: /usr/src/app/ui
        - name: PROXY_URL
          value: https://ngingress-nginx-ingress-controller:443
        - name: VITE_GOOGLE_OAUTH_CLIENT_ID
          value: 
        - name: VITE_MICROSOFT_OAUTH_CLIENT_ID
          value: 
        - name: VITE_GOOGLE_MAP_API_KEY
          value: 
        - name: VITE_GANALYTICS_ID
          value: 
        - name: VITE_HOTJAR_ID
          value: 
        readinessProbe:
          tcpSocket:
            port: 4000
            initialDelaySeconds: 5
            periodSeconds: 10
---
# Source: opticloud/templates/ohsu.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ohsu
  labels:
    app: ohsu
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ohsu
      release: release-name
  template:
    metadata:
      labels:
        app: ohsu
        release: release-name
    spec:
      initContainers:
      - name: "wait-etcd-client"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z etcd-client.default.svc.cluster.local 2379 -w 2; do sleep 2; done']
      containers:
      - name: ohsu
        image: "ohsu:local"
        imagePullPolicy: Never
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
---
# Source: opticloud/templates/organization_violations.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: organization-violations-worker
  labels:
    app: organization-violations-worker
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: organization-violations-worker
      release: release-name
  template:
    metadata:
      labels:
        app: organization-violations-worker
        release: release-name
    spec:
      initContainers:
      - name: "wait-rabbitmq"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
      - name: "wait-restapi"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      containers:
      - name: organization-violations-worker
        image: "organization_violations:local"
        imagePullPolicy: Never
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
---
# Source: opticloud/templates/pharos_receiver.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pharos-receiver
  labels:
    app: pharos-receiver
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: pharos-receiver
      release: release-name
  serviceName: pharos-receiver
  template:
    metadata:
      labels:
        app: pharos-receiver
        release: release-name
    spec:
      initContainers:
      - name: "wait-etcd-client"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z etcd-client.default.svc.cluster.local 2379 -w 2; do sleep 2; done']
      - name: "wait-rabbitmq"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
      containers:
      - name: pharos-receiver
        image: "pharos_receiver:local"
        imagePullPolicy: Never
        ports:
        - containerPort: 8946
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
---
# Source: opticloud/templates/pharos_worker.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pharos-worker
  labels:
    app: pharos-worker
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: pharos-worker
      release: release-name
  template:
    metadata:
      labels:
        app: pharos-worker
        release: release-name
    spec:
      initContainers:
      - name: "wait-etcd-client"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z etcd-client.default.svc.cluster.local 2379 -w 2; do sleep 2; done']
      - name: "wait-rabbitmq"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
      containers:
      - name: pharos-worker
        image: "pharos_worker:local"
        imagePullPolicy: Never
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
---
# Source: opticloud/templates/power_schedule.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: power-schedule-worker
  labels:
    app: power-schedule-worker
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: power-schedule-worker
      release: release-name
  template:
    metadata:
      labels:
        app: power-schedule-worker
        release: release-name
    spec:
      initContainers:
      - name: "wait-rabbitmq"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
      - name: "wait-restapi"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      - name: "wait-mongo"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z mongo.default.svc.cluster.local 80 -w 2; do sleep 2; done']

      containers:
      - name: power-schedule-worker
        image: "power_schedule:local"
        imagePullPolicy: Never
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
---
# Source: opticloud/templates/resource_discovery.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: resource-discovery-worker
  labels:
    app: resource-discovery-worker
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 2
  selector:
    matchLabels:
      app: resource-discovery-worker
      release: release-name
  template:
    metadata:
      labels:
        app: resource-discovery-worker
        release: release-name
    spec:
      initContainers:
      - name: "wait-rabbitmq"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
      - name: "wait-restapi"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      containers:
      - name: resource-discovery-worker
        image: "resource_discovery:local"
        imagePullPolicy: Never
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
        - name: FAKE_CAD_ENABLED
          value: "0"
        resources:
          limits:
            memory: 1024Mi
---
# Source: opticloud/templates/resource_observer.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: resource-observer-worker
  labels:
    app: resource-observer-worker
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: resource-observer-worker
      release: release-name
  template:
    metadata:
      labels:
        app: resource-observer-worker
        release: release-name
    spec:
      initContainers:
      - name: "wait-rabbitmq"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
      - name: "wait-restapi"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      containers:
      - name: resource-observer-worker
        image: "resource_observer:local"
        imagePullPolicy: Never
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
---
# Source: opticloud/templates/resource_violations.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: resource-violations-worker
  labels:
    app: resource-violations-worker
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: resource-violations-worker
      release: release-name
  template:
    metadata:
      labels:
        app: resource-violations-worker
        release: release-name
    spec:
      initContainers:
      - name: "wait-rabbitmq"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
      - name: "wait-restapi"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      containers:
      - name: resource-violations-worker
        image: "resource_violations:local"
        imagePullPolicy: Never
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
---
# Source: opticloud/templates/restapi.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: restapi
  labels:
    app: restapi
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: restapi
      release: release-name
  serviceName: restapi
  template:
    metadata:
      labels:
        app: restapi
        release: release-name
    spec:
      initContainers:
      - name: "wait-etcd-client"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z etcd-client.default.svc.cluster.local 2379 -w 2; do sleep 2; done']
      - name: wait-mariadb
        image: "mariadb:local"
        imagePullPolicy: Never
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mariadb-secret
              key: password
        command: ['sh', '-c', 'until mysql --connect-timeout=2 -h mariadb.default.svc.cluster.local -p$MYSQL_ROOT_PASSWORD -e "SELECT 1"; do sleep 2; done']
      - name: "wait-rabbitmq"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
      containers:
      - name: restapi
        image: "rest_api:local"
        imagePullPolicy: Never
        ports:
        - containerPort: 8999
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
        - name: REQUESTS_CA_BUNDLE
          value: /etc/ssl/certs/ca-certificates.crt
        - name: FAKE_CAD_ENABLED
          value: "0"
        readinessProbe:
          tcpSocket:
            port: 8999
            initialDelaySeconds: 5
            periodSeconds: 10
---
# Source: opticloud/templates/risp_worker.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: risp-worker
  labels:
    app: risp-worker
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: risp-worker
      release: release-name
  template:
    metadata:
      labels:
        app: risp-worker
        release: release-name
    spec:
      initContainers:
      - name: "wait-rabbitmq"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
      - name: "wait-etcd-client"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z etcd-client.default.svc.cluster.local 2379 -w 2; do sleep 2; done']
      - name: "wait-mongo"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z mongo.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      - name: "wait-restapi"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      - name: "wait-clickhouse"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z clickhouse.default.svc.cluster.local 9000 -w 2; do sleep 2; done']
      containers:
      - name: risp-worker
        image: "risp_worker:local"
        imagePullPolicy: Never
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
---
# Source: opticloud/templates/slacker.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: slacker
  labels:
    app: slacker
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  strategy:
    type: Recreate
    rollingUpdate: null
  selector:
    matchLabels:
      app: slacker
      release: release-name
  serviceName: slacker
  template:
    metadata:
      labels:
        app: slacker
        release: release-name
    spec:
      initContainers:
      - name: "wait-mariadb"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z mariadb.default.svc.cluster.local 3306 -w 2; do sleep 2; done']
      - name: "wait-auth"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z auth.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      - name: "wait-restapi"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      containers:
      - name: slacker
        image: "slacker:local"
        imagePullPolicy: Never
        ports:
        - containerPort: 80
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
        - name: SLACK_SIGNING_SECRET
          value: 
        - name: SLACK_CLIENT_ID
          value: 
        - name: SLACK_CLIENT_SECRET
          value: 
        readinessProbe:
          tcpSocket:
            port: 80
            initialDelaySeconds: 5
            periodSeconds: 10
---
# Source: opticloud/templates/slacker_executor.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: slacker-executor
  labels:
    app: slacker-executor
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: slacker-executor
      release: release-name
  template:
    metadata:
      labels:
        app: slacker-executor
        release: release-name
    spec:
      initContainers:
      - name: "wait-rabbitmq"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
      - name: "wait-slacker"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z slacker.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      containers:
      - name: slacker-executor
        image: "slacker_executor:local"
        imagePullPolicy: Never
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
---
# Source: opticloud/templates/thanos_query.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: thanos-query
  labels:
    app: thanos-query
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: thanos-query
      release: release-name
  serviceName: thanos-query
  template:
    metadata:
      labels:
        app: thanos-query
        release: release-name
    spec:
      initContainers:
      - name: "wait-thanos-receive"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z thanos-receive.default.svc.cluster.local 10902 -w 2; do sleep 2; done']
      - name: "wait-thanos-storegateway"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z thanos-storegateway.default.svc.cluster.local 10902 -w 2; do sleep 2; done']
      containers:
        - name: query
          image: "thanosio/thanos:v0.31.0"
          imagePullPolicy: "IfNotPresent"
          command:
            - thanos
          args:
            - query
            - --grpc-address=0.0.0.0:10901
            - --http-address=0.0.0.0:10902
            - --endpoint=dnssrv+_grpc._tcp.thanos-storegateway.default.svc.cluster.local
            - --endpoint=dnssrv+_grpc._tcp.thanos-receive.default.svc.cluster.local
          ports:
            - name: http
              containerPort: 10902
              protocol: TCP
            - name: grpc
              containerPort: 10901
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /-/healthy
              port: http
              initialDelaySeconds: 30
              timeoutSeconds: 30
              periodSeconds: 10
              successThreshold: 1
              failureThreshold: 6
          readinessProbe:
            httpGet:
              path: /-/ready
              port: http
              initialDelaySeconds: 30
              timeoutSeconds: 30
              periodSeconds: 10
              successThreshold: 1
              failureThreshold: 6
---
# Source: opticloud/templates/trapper_worker.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: trapper-worker
  labels:
    app: trapper-worker
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: trapper-worker
      release: release-name
  template:
    metadata:
      labels:
        app: trapper-worker
        release: release-name
    spec:
      initContainers:
      - name: "wait-rabbitmq"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
      - name: "wait-etcd-client"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z etcd-client.default.svc.cluster.local 2379 -w 2; do sleep 2; done']
      - name: "wait-mongo"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z mongo.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      - name: "wait-restapi"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      - name: "wait-clickhouse"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z clickhouse.default.svc.cluster.local 9000 -w 2; do sleep 2; done']
      containers:
      - name: trapper-worker
        image: "trapper_worker:local"
        imagePullPolicy: Never
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
---
# Source: opticloud/templates/webhook_executor.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webhook-executor
  labels:
    app: webhook-executor
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webhook-executor
      release: release-name
  template:
    metadata:
      labels:
        app: webhook-executor
        release: release-name
    spec:
      initContainers:
      - name: "wait-rabbitmq"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
      - name: "wait-restapi"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      - name: "wait-mongo"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z mongo.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      containers:
      - name: webhook-executor
        image: "webhook_executor:local"
        imagePullPolicy: Never
        env:
        - name: IMAGE_ID
          value: 
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
---
# Source: opticloud/templates/clickhouse/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: clickhouse
  labels:
    heritage: Helm
    release: release-name
    app: clickhouse
spec:
  podManagementPolicy: OrderedReady
  replicas: 1
  selector:
    matchLabels:
      app: clickhouse
      release: release-name
  serviceName: clickhouse
  template:
    metadata:
      labels:
        app: clickhouse
        release: release-name
    spec:
      containers:
        - name: clickhouse
          image: "yandex/clickhouse-server:21.10"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8123
            - name: native
              containerPort: 9000
          livenessProbe:
            timeoutSeconds: 1
            initialDelaySeconds: 30
            tcpSocket:
              port: 9000
          readinessProbe:
            timeoutSeconds: 1
            initialDelaySeconds: 5
            tcpSocket:
              port: 9000
          volumeMounts:
            - name: clickhouse-data-volume
              mountPath: /var/lib/clickhouse/
            - name: config-volume
              mountPath: /etc/clickhouse-server/
      volumes:
        - name: clickhouse-data-volume
          hostPath:
            path: /opticloud/clickhouse
            type: DirectoryOrCreate
        - name: config-volume
          configMap:
            name: clickhouse-config
---
# Source: opticloud/templates/etcd.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: etcd
  labels:
    app: etcd
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: etcd
      release: release-name
  serviceName: etcd-client
  template:
    metadata:
      labels:
        app: etcd
        release: release-name
    spec:
      containers:
      - name: etcd
        image: "etcd:local"
        imagePullPolicy: Never
        command:
          - "/usr/local/bin/etcd"
          - "-listen-client-urls"
          - "http://0.0.0.0:2379"
          - "-advertise-client-urls"
          - "http://0.0.0.0:2379"
        ports:
        - name: client
          containerPort: 2379
        - name: peer
          containerPort: 2380
        readinessProbe:
          exec:
            command:
            - sh
            - -ec
            - |
              etcdctl --endpoints=http://[127.0.0.1]:2379 cluster-health
          initialDelaySeconds: 5
          periodSeconds: 10
          timeoutSeconds: 20
        volumeMounts:
        - name: etcd-persistent-storage
          mountPath: /default.etcd
          readOnly: false
          mountPropagation: HostToContainer
        env:
        - name: INITIAL_CLUSTER_SIZE
          value: "1"
        - name: SET_NAME
          value: etcd-cluster
        - name: IMAGE_ID
          value: 
        - name: ETCD_HEARTBEAT_INTERVAL
          value: "500"
        - name: ETCD_ELECTION_TIMEOUT
          value: "5000"
      volumes:
      - name: etcd-persistent-storage
        hostPath:
          path: /opticloud/etcd
          type: DirectoryOrCreate
      nodeSelector:
        node-role.kubernetes.io/master:
---
# Source: opticloud/templates/influxdb.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: influxdb
  labels:
    app: influxdb
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: influxdb
      release: release-name
  serviceName: influxdb
  template:
    metadata:
      labels:
        app: influxdb
        release: release-name
    spec:
      containers:
      - name: influxdb
        image: "influxdb:local"
        imagePullPolicy: Never
        ports:
        - containerPort: 8086
        volumeMounts:
        - name: influxdb-storage
          mountPath: /var/lib/influxdb
          readOnly: false
          mountPropagation: HostToContainer
        readinessProbe:
          httpGet:
            path: /ping
            port: 8086
        env:
        - name: IMAGE_ID
          value: 
      volumes:
      - name: influxdb-storage
        hostPath:
          path: /opticloud/influxdb
          type: DirectoryOrCreate
      nodeSelector:
        node-role.kubernetes.io/master:
---
# Source: opticloud/templates/mariadb.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mariadb
  labels:
    app: mariadb
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mariadb
      release: release-name
  serviceName: mariadb
  template:
    metadata:
      labels:
        app: mariadb
        release: release-name
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - topologyKey: "kubernetes.io/hostname"
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - mariadb
      initContainers:
      - name: copy-mariadb-config
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'cp /configmap/* /etc/mysql/conf.d']
        volumeMounts:
        - name: configmap
          mountPath: /configmap
        - name: config
          mountPath: /etc/mysql/conf.d
      containers:
      - name: mariadb
        image: "mariadb:local"
        imagePullPolicy: Never
        env:
        - name: MARIADB_CNF_SHA
          value: 0a9e2bd19a9e03d175f16ebcb7ea383d37a4e965c9f5f326487a0174d988ba9d
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: IMAGE_ID
          value: 
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mariadb-secret
              key: password
        ports:
        - name: mariadb
          containerPort: 3306
        args: ["--max_connections=4096"]
        readinessProbe:
          exec:
            command:
            - bash
            - -c
            - |
              mysql -h 127.0.0.1 -e "SELECT 1" -p$MYSQL_ROOT_PASSWORD
          initialDelaySeconds: 5
          periodSeconds: 10
          timeoutSeconds: 20
        volumeMounts:
        - name: mariadb-persistent-storage
          mountPath: /var/lib/mysql
          readOnly: false
        - name: config
          mountPath: /etc/mysql/conf.d
      volumes:
      - name: mariadb-persistent-storage
        hostPath:
          path: /opticloud/db
          type: DirectoryOrCreate
      - name: config
        emptyDir: {}
      - name: configmap
        configMap:
          name: mysql-config-vol
          items:
          - path: "mariadb.cnf"
            key: mariadb.cnf
---
# Source: opticloud/templates/minio.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: minio
  labels:
    app: minio
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: minio
      release: release-name
  serviceName: minio
  template:
    metadata:
      labels:
        app: minio
        release: release-name
    spec:
      initContainers:
      containers:
      - name: "minio"
        image: "minio/minio:RELEASE.2020-03-19T21-49-00Z"
        imagePullPolicy: IfNotPresent
        env:
        - name: IMAGE_ID
          value: 
        - name: MINIO_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: "minio-secret"
              key: access
        - name: MINIO_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: "minio-secret"
              key: secret
        ports:
        - name: minio
          containerPort: 9000
        args:
        - "server"
        - "/data"
        volumeMounts:
        - name: minio-persistent-storage
          mountPath: /data
          readOnly: false
        readinessProbe:
          tcpSocket:
            port: 9000
            initialDelaySeconds: 5
            periodSeconds: 10
      volumes:
      - name: minio-persistent-storage
        hostPath:
          path: /opticloud/minio
          type: DirectoryOrCreate
---
# Source: opticloud/templates/mongodb.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mongo
  labels:
    app: mongo
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongo
      release: release-name
  serviceName: "mongo-discovery"
  template:
    metadata:
      labels:
        app: mongo
        release: release-name
    spec:
      terminationGracePeriodSeconds: 30
      initContainers:
      - name: copy-config
        image: "busybox:1.30.0"
        imagePullPolicy: "IfNotPresent"
        command:
          - "sh"
        args:
          - "-c"
          - |
            set -e
            set -x
            cp /configdb-readonly/mongod.conf /data/configdb/mongod.conf
            cp /keydir-readonly/key.txt /data/configdb/key.txt
            chmod 600 /data/configdb/key.txt
        volumeMounts:
        - name: config
          mountPath: /configdb-readonly
        - name: configdir
          mountPath: /data/configdb
        - name: keydir
          mountPath: /keydir-readonly
      - name: bootstrap
        image: "mongo:local"
        command:
          - peer-finder
        args:
          - -on-start=/on-start.sh
          - "-service=mongo-discovery"
        imagePullPolicy: Never
        env:
        - name: IMAGE_ID
          value: 
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: REPLICA_SET
          value: "mongo"
        - name: TIMEOUT
          value: "900"
        - name: SKIP_INIT
          value: "false"
        - name: TLS_MODE
          value: "requireSSL"
        - name: AUTH
          value: "true"
        - name: ADMIN_USER
          valueFrom:
            secretKeyRef:
              name: "mongo-secret"
              key: username
        - name: ADMIN_PASSWORD
          valueFrom:
            secretKeyRef:
              name: "mongo-secret"
              key: password
        volumeMounts:
        - name: configdir
          mountPath: /data/configdb
        - name: datadir
          mountPath: /data/db
        - name: workdir
          mountPath: /work-dir
      containers:
      - name: mongo
        image: "mongo:local"
        imagePullPolicy: "Never"
        ports:
        - name: mongodb
          containerPort: 27017
        command:
          - mongod
        args:
          - --config=/data/configdb/mongod.conf
          - --dbpath=/data/db
          - --replSet=mongo
          - --port=27017
          - --bind_ip=0.0.0.0
          - --auth
          - --keyFile=/data/configdb/key.txt
          - --storageEngine=wiredTiger
          - --wiredTigerCacheSizeGB=1
        livenessProbe:
          exec:
            command:
              - mongo
              - --eval
              - "db.adminCommand('ping')"
          initialDelaySeconds: 5
          timeoutSeconds: 1
          failureThreshold: 3
          periodSeconds: 10
          successThreshold: 1
        readinessProbe:
          exec:
            command:
              - mongo
              - --eval
              - "db.adminCommand('ping')"
          initialDelaySeconds: 30
          timeoutSeconds: 5
          failureThreshold: 3
          periodSeconds: 10
          successThreshold: 1
        volumeMounts:
        - name: datadir
          mountPath: /data/db
        - name: configdir
          mountPath: /data/configdb
      volumes:
      - name: config
        configMap:
          name: mongo-config
      - name: keydir
        secret:
          defaultMode: 0400
          secretName: mongo-secret
      - name: configdir
        emptyDir: {}
      - name: workdir
        emptyDir: {}
      - name: datadir
        hostPath:
          path: /opticloud/mongo
          type: DirectoryOrCreate
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - topologyKey: "kubernetes.io/hostname"
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - mongo
---
# Source: opticloud/templates/rabbitmq.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: rabbitmq
  labels:
    app: rabbitmq
    chart: opticloud-0.1.0
    release: release-name
spec:
  podManagementPolicy: OrderedReady
  replicas: 1
  selector:
    matchLabels:
      app: rabbitmq
      release: release-name
  serviceName: rabbitmq-discovery
  template:
    metadata:
      labels:
        app: rabbitmq
        release: release-name
    spec:
      terminationGracePeriodSeconds: 10
      serviceAccountName: rabbitmq
      initContainers:
      - name: bootstrap
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh']
        args:
          - "-c"
          - |
            set -ex
            cp /configmap/* /etc/rabbitmq
            echo "${RABBITMQ_ERLANG_COOKIE}" > /var/lib/rabbitmq/.erlang.cookie
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        - name: RABBITMQ_MNESIA_DIR
          value: /var/lib/rabbitmq/mnesia/rabbit@$(POD_NAME).rabbitmq-discovery.default.svc.cluster.local
        - name: RABBITMQ_ERLANG_COOKIE
          valueFrom:
            secretKeyRef:
              name: rabbit-secret
              key: erlang-cookie
        volumeMounts:
        - name: configmap
          mountPath: /configmap
        - name: config
          mountPath: /etc/rabbitmq
        - name: datadir
          mountPath: /var/lib/rabbitmq
        - name: mnesia
          mountPath: /var/lib/rabbitmq/mnesia/
      containers:
      - name: rabbitmq
        image: "rabbitmq:3.8.2-management-alpine"
        imagePullPolicy: IfNotPresent
        env:
        - name: IMAGE_ID
          value: 
        - name: RABBITMQ_DEFAULT_USER
          valueFrom:
            secretKeyRef:
              name: rabbit-secret
              key: username
        - name: RABBITMQ_DEFAULT_PASS
          valueFrom:
            secretKeyRef:
              name: rabbit-secret
              key: password
        - name: MY_POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        - name: RABBITMQ_USE_LONGNAME
          value: "true"
        - name: RABBITMQ_NODENAME
          value: rabbit@$(MY_POD_NAME).rabbitmq-discovery.default.svc.cluster.local
        - name: K8S_HOSTNAME_SUFFIX
          value: .rabbitmq-discovery.default.svc.cluster.local
        - name: K8S_SERVICE_NAME
          value: rabbitmq-discovery
        - name: RABBITMQ_ERLANG_COOKIE
          valueFrom:
            secretKeyRef:
              name: rabbit-secret
              key: erlang-cookie
        - name: RABBIT_MANAGEMENT_USER
          valueFrom:
            secretKeyRef:
              name: rabbit-secret
              key: management-username
        - name: RABBIT_MANAGEMENT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: rabbit-secret
              key: management-password
        - name: CONFIG_HASH
          value: d0e9ddd45e3b7c1d423c81a91d7827b7d3dbcf1a583f9bb3546bc30921ae3d4f
        - name: PLUGINS_HASH
          value: 229a96cd88e88074f5891cfe31305540ee307a4e4113586f98ec39b3bc62c8f6
        ports:
        - name: epmd
          protocol: TCP
          containerPort: 4369
        - name: amqp
          protocol: TCP
          containerPort: 5672
        - name: http
          protocol: TCP
          containerPort: 15672
        livenessProbe:
          initialDelaySeconds: 120
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 6
          exec:
            command:
              - /bin/sh
              - -c
              - 'wget -O - -q --header "Authorization: Basic `echo -n \"$RABBIT_MANAGEMENT_USER:$RABBIT_MANAGEMENT_PASSWORD\" | base64`" http://localhost:15672/api/healthchecks/node | grep -qF "{\"status\":\"ok\"}"'
        readinessProbe:
          initialDelaySeconds: 20
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 6
          exec:
            command:
              - /bin/sh
              - -c
              - 'wget -O - -q --header "Authorization: Basic `echo -n \"$RABBIT_MANAGEMENT_USER:$RABBIT_MANAGEMENT_PASSWORD\" | base64`" http://localhost:15672/api/healthchecks/node | grep -qF "{\"status\":\"ok\"}"'
        volumeMounts:
        - name: datadir
          mountPath: /var/lib/rabbitmq
        - name: mnesia
          mountPath: /var/lib/rabbitmq/mnesia
        - name: config
          mountPath: /etc/rabbitmq
        - name: definitions
          mountPath: /etc/definitions
          readOnly: true
      volumes:
      - name: config
        emptyDir: {}
      - name: configmap
        configMap:
          name: rabbitmq-config
      - name: datadir
        emptyDir: {}
      - name: mnesia
        persistentVolumeClaim:
          claimName: rabbitmq-claim
      - name: definitions
        secret:
          secretName: rabbit-secret
          items:
          - key: definitions.json
            path: definitions.json
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - topologyKey: "kubernetes.io/hostname"
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - rabbitmq
---
# Source: opticloud/templates/redis.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis
  labels:
    app: redis
    chart: opticloud-0.1.0
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      release: release-name
  serviceName: redis
  template:
    metadata:
      labels:
        app: redis
        release: release-name
    spec:
      initContainers:
      containers:
      - name: redis
        image: "redis:local"
        imagePullPolicy: Never
        ports:
        - containerPort: 6379
        readinessProbe:
          exec:
            command:
            - sh
            - -c
            - "redis-cli -h $(hostname) ping"
          initialDelaySeconds: 15
          timeoutSeconds: 5
        env:
        - name: IMAGE_ID
          value:
---
# Source: opticloud/templates/thanos_receive.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: thanos-receive
  labels:
    app: thanos-receive
    release: release-name
spec:
  replicas: 1
  podManagementPolicy: OrderedReady
  serviceName: thanos-receive
  selector:
    matchLabels:
      app: thanos-receive
      release: release-name
  template:
    metadata:
      labels:
        app: thanos-receive
        release: release-name
    spec:
      initContainers:
      - name: "wait-minio"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z minio.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      containers:
        - name: receive
          imagePullPolicy: "IfNotPresent"
          image: "thanosio/thanos:v0.31.0"
          env:
            - name: NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
          ports:
            - containerPort: 10901
              name: grpc
              protocol: TCP
            - containerPort: 10902
              name: http
              protocol: TCP
            - containerPort: 19291
              name: remote
              protocol: TCP
          resources:
            limits:
              memory: 8192Mi
          command:
            - thanos
          args:
            - receive
            - --grpc-address=0.0.0.0:10901
            - --http-address=0.0.0.0:10902
            - --remote-write.address=0.0.0.0:19291
            - --objstore.config-file=/var/thanos/config/thanos_conf.yaml
            - --tsdb.path=/var/thanos/receive
            - --tsdb.retention=4h
            - --tsdb.wal-compression
            - --tsdb.no-lockfile
            # - --tsdb.too-far-in-future.time-window=1h  # TODO: uncomment me later
            - --tsdb.out-of-order.time-window=1h
            - --label=receive="$(NAME)"
            - --receive.tenant-header=Cloud-Account-Id
            - --receive-forward-timeout=2m
            - --log.level=info
            - --receive.limits-config-file=/var/thanos/limits_config/thanos_limits.yaml
            - --tsdb.memory-snapshot-on-shutdown
          livenessProbe:
            httpGet:
              path: /-/healthy
              port: http
            initialDelaySeconds: 30
            timeoutSeconds: 30
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            httpGet:
              path: /-/ready
              port: http
            initialDelaySeconds: 30
            timeoutSeconds: 30
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 6
          volumeMounts:
            - name: data
              mountPath: /var/thanos/receive
            - name: config
              mountPath: /var/thanos/config
              readOnly: true
            - name: limits-config
              mountPath: /var/thanos/limits_config
      volumes:
        - name: config
          secret:
            secretName: thanos-secret
            items:
            - key: thanos_conf.yaml
              path: thanos_conf.yaml
        - name: limits-config
          configMap:
            name: thanos-receive-limits-config
        - name: data
          persistentVolumeClaim:
            claimName: thanos-receive-claim
---
# Source: opticloud/templates/thanos_storegateway.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: thanos-storegateway
  labels:
    app: thanos-storegateway
    release: release-name
spec:
  replicas: 1
  podManagementPolicy: OrderedReady
  serviceName: thanos-storegateway
  selector:
    matchLabels:
      app: thanos-storegateway
      release: release-name
  template:
    metadata:
      labels:
        app: thanos-storegateway
        release: release-name
    spec:
      initContainers:
      - name: "wait-minio"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z minio.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      containers:
        - name: storegateway
          image: "thanosio/thanos:v0.31.0"
          imagePullPolicy: "IfNotPresent"
          command:
            - thanos
          args:
            - store
            - --grpc-address=0.0.0.0:10901
            - --http-address=0.0.0.0:10902
            - --ignore-deletion-marks-delay=3h
            - --objstore.config-file=/var/thanos/config/thanos_conf.yaml
            - --data-dir=/var/thanos/data
          ports:
            - containerPort: 10901
              name: grpc
              protocol: TCP
            - containerPort: 10902
              name: http
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /-/healthy
              port: http
            initialDelaySeconds: 30
            timeoutSeconds: 30
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            httpGet:
              path: /-/ready
              port: http
            initialDelaySeconds: 30
            timeoutSeconds: 30
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 6
          volumeMounts:
            - name: data
              mountPath: /var/thanos/data
            - name: config
              mountPath: /var/thanos/config
              readOnly: true
      volumes:
        - name: config
          secret:
            secretName: thanos-secret
            items:
              - key: thanos_conf.yaml
                path: thanos_conf.yaml
        - name: data
          persistentVolumeClaim:
            claimName: thanos-storegateway-claim
---
# Source: opticloud/templates/thanos_web.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: thanos-web
  labels:
    app: thanos-web
    release: release-name
spec:
  replicas: 1
  podManagementPolicy: OrderedReady
  serviceName: thanos-web
  selector:
    matchLabels:
      app: thanos-web
      release: release-name
  template:
    metadata:
      labels:
        app: thanos-web
        release: release-name
    spec:
      initContainers:
      - name: "wait-minio"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z minio.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      containers:
        - name: web
          image: "thanosio/thanos:v0.31.0"
          imagePullPolicy: "IfNotPresent"
          command:
            - thanos
          args:
            - tools
            - bucket
            - web
            - --http-address=0.0.0.0:10902
            - --objstore.config-file=/var/thanos/config/thanos_conf.yaml
            - --refresh=10m
          ports:
            - containerPort: 10902
              name: http
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /-/healthy
              port: http
            initialDelaySeconds: 30
            timeoutSeconds: 30
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            httpGet:
              path: /-/ready
              port: http
            initialDelaySeconds: 30
            timeoutSeconds: 30
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 6
          volumeMounts:
            - name: config
              mountPath: /var/thanos/config
              readOnly: true
      volumes:
        - name: config
          secret:
            secretName: thanos-secret
            items:
              - key: thanos_conf.yaml
                path: thanos_conf.yaml
---
# Source: opticloud/templates/etcd.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: pre-configurator
  annotations:
    checksum/config: fee04ecd9cfde5585c21438910a5ba1c670da8a2a4fbf35227a0e7cba908dab4
  labels:
    app: configurator
    chart: opticloud-0.1.0
    release: release-name
spec:
  template:
    metadata:
      name: pre-configurator
      labels:
        app: configurator
        release: release-name
    spec:
      containers:
      - name: configurator
        image: configurator:local
        volumeMounts:
        - name: config-volume
          mountPath: /config
        env:
        - name: HX_ETCD_HOST
          value: etcd-client
        - name: HX_ETCD_PORT
          value: "2379"
      initContainers:
      - name: "wait-etcd-client"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z etcd-client.default.svc.cluster.local 2379 -w 2; do sleep 2; done']
      - name: wait-mariadb
        image: "mariadb:local"
        imagePullPolicy: Never
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mariadb-secret
              key: password
        command: ['sh', '-c', 'until mysql --connect-timeout=2 -h mariadb.default.svc.cluster.local -p$MYSQL_ROOT_PASSWORD -e "SELECT 1"; do sleep 2; done']
      - name: "wait-mongo"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z mongo.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      - name: "wait-rabbitmq"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
      - name: "wait-influxdb"
        image: "busybox:1.30.0"
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -z influxdb.default.svc.cluster.local 80 -w 2; do sleep 2; done']
      restartPolicy: Never
      volumes:
      - name: config-volume
        configMap:
          name: opticloud-etcd
      nodeSelector:
        node-role.kubernetes.io/master:
  backoffLimit: 10
---
# Source: opticloud/templates/bi_scheduler.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: bi-scheduler
  labels:
    app: bi-scheduler
    chart: opticloud-0.1.0
    release: release-name
spec:
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  schedule: "*/5 * * * *"
  startingDeadlineSeconds: 100
  jobTemplate:
    metadata:
      labels:
        app: bi-scheduler
        release: release-name
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            app: bi-scheduler
            release: release-name
        spec:
          initContainers:
          - name: "wait-rabbitmq"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
          - name: "wait-restapi"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
          containers:
          - name: bi-scheduler
            image: "bi_scheduler:local"
            imagePullPolicy: Never
            command:
            - "python3"
            - "docker_images/bi_scheduler/scheduler.py"
            env:
            - name: IMAGE_ID
              value: 
            - name: HX_ETCD_HOST
              value: etcd-client
            - name: HX_ETCD_PORT
              value: "2379"
          restartPolicy: Never
---
# Source: opticloud/templates/booking_observer.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: booking-observer-scheduler
  labels:
    app: booking-observer-scheduler
    chart: opticloud-0.1.0
    release: release-name
spec:
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  schedule: "*/1 * * * *"
  startingDeadlineSeconds: 100
  jobTemplate:
    metadata:
      labels:
        app: booking-observer-scheduler
        release: release-name
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            app: booking-observer-scheduler
            release: release-name
        spec:
          initContainers:
          - name: "wait-rabbitmq"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
          - name: "wait-restapi"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
          containers:
          - name: booking-observer-scheduler
            image: "booking_observer:local"
            imagePullPolicy: Never
            command:
            - "python3"
            - "docker_images/booking_observer/scheduler.py"
            env:
            - name: HX_ETCD_HOST
              value: etcd-client
            - name: HX_ETCD_PORT
              value: "2379"
          restartPolicy: Never
---
# Source: opticloud/templates/calendar_observer.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: calendar-observer-scheduler
  labels:
    app: calendar-observer-scheduler
    chart: opticloud-0.1.0
    release: release-name
spec:
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  schedule: "0 * * * *"
  startingDeadlineSeconds: 100
  jobTemplate:
    metadata:
      labels:
        app: calendar-observer-scheduler
        release: release-name
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            app: calendar-observer-scheduler
            release: release-name
        spec:
          initContainers:
          - name: "wait-restapi"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
          containers:
          - name: calendar-observer-scheduler
            image: "calendar_observer:local"
            imagePullPolicy: Never
            command:
            - "python3"
            - "docker_images/calendar_observer/scheduler.py"
            env:
            - name: HX_ETCD_HOST
              value: etcd-client
            - name: HX_ETCD_PORT
              value: "2379"
          restartPolicy: Never
---
# Source: opticloud/templates/cleaninfluxdb.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cleaninfluxdb
  labels:
    app: cleaninfluxdb
    chart: opticloud-0.1.0
    release: release-name
spec:
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  schedule: "@weekly"
  startingDeadlineSeconds: 100
  jobTemplate:
    metadata:
      labels:
        app: cleaninfluxdb
        release: release-name
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            app: cleaninfluxdb
            release: release-name
        spec:
          initContainers:
          - name: "wait-influxdb"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z influxdb.default.svc.cluster.local 80 -w 2; do sleep 2; done']
          containers:
          - name: cleaninfluxdb
            image: "cleaninfluxdb:local"
            env:
            - name: HX_INFLUXDB_HOST
              value: "influxdb"
            - name: HX_INFLUXDB_PORT
              value: "80"
            - name: HX_DAYS_TO_STORE
              value: "90"
          restartPolicy: Never
---
# Source: opticloud/templates/cleanmongodb.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cleanmongodb
  labels:
    app: cleanmongodb
    chart: opticloud-0.1.0
    release: release-name
spec:
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  schedule: "*/3 * * * *"
  startingDeadlineSeconds: 100
  jobTemplate:
    metadata:
      labels:
        app: cleanmongodb
        release: release-name
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            app: cleanmongodb
            release: release-name
        spec:
          initContainers:
          - name: "wait-mariadb"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z mariadb.default.svc.cluster.local 3306 -w 2; do sleep 2; done']
          - name: "wait-mongo"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z mongo.default.svc.cluster.local 80 -w 2; do sleep 2; done']
          containers:
          - name: cleanmongodb
            image: "cleanmongodb:local"
            imagePullPolicy: Never
            env:
            - name: IMAGE_ID
              value: 
            - name: HX_ETCD_HOST
              value: etcd-client
            - name: HX_ETCD_PORT
              value: "2379"
            volumeMounts:
              - name: cleanmongodb-data-volume
                mountPath: /src/archive/
          restartPolicy: Never
          volumes:
            - name: cleanmongodb-data-volume
              hostPath:
                path: /opticloud/cleanmongodb/archive
                type: DirectoryOrCreate
---
# Source: opticloud/templates/demo_org_cleanup.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: demoorgcleanup
  labels:
    app: demoorgcleanup
    chart: opticloud-0.1.0
    release: release-name
spec:
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  schedule: "0 0 * * *"
  startingDeadlineSeconds: 100
  jobTemplate:
    metadata:
      labels:
        app: demoorgcleanup
        release: release-name
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            app: demoorgcleanup
            release: release-name
        spec:
          initContainers:
          - name: "wait-restapi"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
          containers:
          - name: demoorgcleanup
            image: "demo_org_cleanup:local"
            imagePullPolicy: Never
            env:
            - name: IMAGE_ID
              value: 
            - name: HX_ETCD_HOST
              value: etcd-client
            - name: HX_ETCD_PORT
              value: "2379"
          restartPolicy: Never
---
# Source: opticloud/templates/failed_imports_dataset_generator.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: failed-imports-dataset-generator
  labels:
    app: failed-imports-dataset-generator
    chart: opticloud-0.1.0
    release: release-name
spec:
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  schedule: "0 0 * * *"
  startingDeadlineSeconds: 100
  jobTemplate:
    metadata:
      labels:
        app: failed-imports-dataset-generator
        release: release-name
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            app: failed-imports-dataset-generator
            release: release-name
        spec:
          initContainers:
          - name: "wait-mariadb"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z mariadb.default.svc.cluster.local 3306 -w 2; do sleep 2; done']
          containers:
          - name: failed-imports-dataset-generator
            image: "failed_imports_dataset_generator:local"
            imagePullPolicy: Never
            env:
            - name: IMAGE_ID
              value: 
            - name: HX_ETCD_HOST
              value: etcd-client
            - name: HX_ETCD_PORT
              value: "2379"
          restartPolicy: Never
---
# Source: opticloud/templates/gemini_scheduler.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: gemini-scheduler
  labels:
    app: gemini-scheduler
    chart: opticloud-0.1.0
    release: release-name
spec:
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  schedule: "*/5 * * * *"
  startingDeadlineSeconds: 100
  jobTemplate:
    metadata:
      labels:
        app: gemini-scheduler
        release: release-name
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            app: gemini-scheduler
            release: release-name
        spec:
          initContainers:
          - name: "wait-rabbitmq"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
          - name: "wait-etcd-client"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z etcd-client.default.svc.cluster.local 2379 -w 2; do sleep 2; done']
          - name: "wait-restapi"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
          containers:
          - name: gemini-scheduler
            image: "gemini_scheduler:local"
            imagePullPolicy: Never
            env:
            - name: IMAGE_ID
              value: 
            - name: HX_ETCD_HOST
              value: etcd-client
            - name: HX_ETCD_PORT
              value: "2379"
          restartPolicy: Never
---
# Source: opticloud/templates/insider_scheduler.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: insider-scheduler
  labels:
    app: insider-scheduler
    chart: opticloud-0.1.0
    release: release-name
spec:
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  schedule: "0 0 * * *"
  startingDeadlineSeconds: 100
  jobTemplate:
    metadata:
      labels:
        app: insider-scheduler
        release: release-name
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            app: insider-scheduler
            release: release-name
        spec:
          initContainers:
          - name: "wait-rabbitmq"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
          - name: "wait-etcd-client"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z etcd-client.default.svc.cluster.local 2379 -w 2; do sleep 2; done']
          - name: "wait-mongo"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z mongo.default.svc.cluster.local 80 -w 2; do sleep 2; done']
          containers:
          - name: insider-scheduler
            image: "insider_scheduler:local"
            imagePullPolicy: Never
            env:
            - name: IMAGE_ID
              value: 
            - name: HX_ETCD_HOST
              value: etcd-client
            - name: HX_ETCD_PORT
              value: "2379"
          restartPolicy: Never
---
# Source: opticloud/templates/layout_cleaner.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: layout-cleaner
  labels:
    app: layout-cleaner
    chart: opticloud-0.1.0
    release: release-name
spec:
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  schedule: "0 3 * * *"
  startingDeadlineSeconds: 100
  jobTemplate:
    metadata:
      labels:
        app: layout-cleaner
        release: release-name
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            app: layout-cleaner
            release: release-name
        spec:
          initContainers:
          - name: "wait-restapi"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
          containers:
          - name: layout-cleaner
            image: "layout_cleaner:local"
            imagePullPolicy: Never
            command:
            - "python3"
            - "docker_images/layout_cleaner/worker.py"
            env:
            - name: IMAGE_ID
              value: 
            - name: HX_ETCD_HOST
              value: etcd-client
            - name: HX_ETCD_PORT
              value: "2379"
          restartPolicy: Never
---
# Source: opticloud/templates/live_demo_generator.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: live-demo-generator-scheduler
  labels:
    app: live-demo-generator-scheduler
    chart: opticloud-0.1.0
    release: release-name
spec:
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  schedule: "0 * * * *"
  startingDeadlineSeconds: 100
  jobTemplate:
    metadata:
      labels:
        app: live-demo-generator-scheduler
        release: release-name
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            app: live-demo-generator-scheduler
            release: release-name
        spec:
          initContainers:
          - name: "wait-rabbitmq"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
          - name: "wait-mongo"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z mongo.default.svc.cluster.local 80 -w 2; do sleep 2; done']
          containers:
          - name: live-demo-generator-scheduler
            image: "live_demo_generator:local"
            imagePullPolicy: Never
            command:
            - "python3"
            - "docker_images/live_demo_generator/scheduler.py"
            env:
            - name: HX_ETCD_HOST
              value: etcd-client
            - name: HX_ETCD_PORT
              value: "2379"
          restartPolicy: Never
---
# Source: opticloud/templates/metroculus_scheduler.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: metroculusscheduler
  labels:
    app: metroculusscheduler
    chart: opticloud-0.1.0
    release: release-name
spec:
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  schedule: "*/30 * * * *"
  startingDeadlineSeconds: 100
  jobTemplate:
    metadata:
      labels:
        app: metroculusscheduler
        release: release-name
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            app: metroculusscheduler
            release: release-name
        spec:
          initContainers:
          - name: "wait-rabbitmq"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
          - name: "wait-etcd-client"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z etcd-client.default.svc.cluster.local 2379 -w 2; do sleep 2; done']
          - name: "wait-restapi"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
          containers:
          - name: metroculusscheduler
            image: "metroculus_scheduler:local"
            imagePullPolicy: Never
            env:
            - name: IMAGE_ID
              value: 
            - name: HX_ETCD_HOST
              value: etcd-client
            - name: HX_ETCD_PORT
              value: "2379"
          restartPolicy: Never
---
# Source: opticloud/templates/organization_violations.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: organization-violations-scheduler
  labels:
    app: organization-violations-scheduler
    chart: opticloud-0.1.0
    release: release-name
spec:
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  schedule: "*/5 * * * *"
  startingDeadlineSeconds: 100
  jobTemplate:
    metadata:
      labels:
        app: organization-violations-scheduler
        release: release-name
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            app: organization-violations-scheduler
            release: release-name
        spec:
          initContainers:
          - name: "wait-rabbitmq"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
          - name: "wait-restapi"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
          containers:
          - name: organization-violations-scheduler
            image: "organization_violations:local"
            imagePullPolicy: Never
            command:
            - "python3"
            - "docker_images/organization_violations/scheduler.py"
            env:
            - name: HX_ETCD_HOST
              value: etcd-client
            - name: HX_ETCD_PORT
              value: "2379"
          restartPolicy: Never
---
# Source: opticloud/templates/power_schedule.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: power-schedule-scheduler
  labels:
    app: power-schedule-scheduler
    chart: opticloud-0.1.0
    release: release-name
spec:
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  schedule: "*/5 * * * *"
  startingDeadlineSeconds: 100
  jobTemplate:
    metadata:
      labels:
        app: power-schedule-scheduler
        release: release-name
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            app: power-schedule-scheduler
            release: release-name
        spec:
          initContainers:
          - name: "wait-rabbitmq"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
          - name: "wait-restapi"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
          containers:
          - name: power-schedule-scheduler
            image: "power_schedule:local"
            imagePullPolicy: Never
            command:
            - "python3"
            - "docker_images/power_schedule/scheduler.py"
            env:
            - name: HX_ETCD_HOST
              value: etcd-client
            - name: HX_ETCD_PORT
              value: "2379"
          restartPolicy: Never
---
# Source: opticloud/templates/report_import_scheduler.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: report-import-scheduler-0
  labels:
    app: report-import-scheduler-0
    chart: opticloud-0.1.0
    release: release-name
spec:
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  schedule: "*/15 * * * *"
  startingDeadlineSeconds: 100
  jobTemplate:
    metadata:
      labels:
        app: report-import-scheduler-0
        release: release-name
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            app: report-import-scheduler-0
            release: release-name
        spec:
          initContainers:
          - name: "wait-restapi"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
          containers:
          - name: report-import-scheduler-0
            image: "cleanelkdb:local"
            imagePullPolicy: Never
            env:
            - name: PERIOD
              value: "0"
            - name: CLUSTER_SECRET
              valueFrom:
                secretKeyRef:
                  name: cluster-secret
                  key: cluster_secret
            command:
            - "/bin/sh"
            - "-ec"
            - |
              set -x
              
              curl -X POST -f -H "Secret: $CLUSTER_SECRET" -d '{"period": '"$PERIOD"'}' \
                http://restapi:80/restapi/v2/schedule_imports
          restartPolicy: Never
---
# Source: opticloud/templates/report_import_scheduler.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: report-import-scheduler-1
  labels:
    app: report-import-scheduler-1
    chart: opticloud-0.1.0
    release: release-name
spec:
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  schedule: "0 * * * *"
  startingDeadlineSeconds: 100
  jobTemplate:
    metadata:
      labels:
        app: report-import-scheduler-1
        release: release-name
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            app: report-import-scheduler-1
            release: release-name
        spec:
          initContainers:
          - name: "wait-restapi"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
          containers:
          - name: report-import-scheduler-1
            image: "cleanelkdb:local"
            imagePullPolicy: Never
            env:
            - name: PERIOD
              value: "1"
            - name: CLUSTER_SECRET
              valueFrom:
                secretKeyRef:
                  name: cluster-secret
                  key: cluster_secret
            command:
            - "/bin/sh"
            - "-ec"
            - |
              set -x
              
              curl -X POST -f -H "Secret: $CLUSTER_SECRET" -d '{"period": '"$PERIOD"'}' \
                http://restapi:80/restapi/v2/schedule_imports
          restartPolicy: Never
---
# Source: opticloud/templates/report_import_scheduler.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: report-import-scheduler-24
  labels:
    app: report-import-scheduler-24
    chart: opticloud-0.1.0
    release: release-name
spec:
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  schedule: "0 0 * * *"
  startingDeadlineSeconds: 100
  jobTemplate:
    metadata:
      labels:
        app: report-import-scheduler-24
        release: release-name
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            app: report-import-scheduler-24
            release: release-name
        spec:
          initContainers:
          - name: "wait-restapi"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
          containers:
          - name: report-import-scheduler-24
            image: "cleanelkdb:local"
            imagePullPolicy: Never
            env:
            - name: PERIOD
              value: "24"
            - name: CLUSTER_SECRET
              valueFrom:
                secretKeyRef:
                  name: cluster-secret
                  key: cluster_secret
            command:
            - "/bin/sh"
            - "-ec"
            - |
              set -x
              
              curl -X POST -f -H "Secret: $CLUSTER_SECRET" -d '{"period": '"$PERIOD"'}' \
                http://restapi:80/restapi/v2/schedule_imports
          restartPolicy: Never
---
# Source: opticloud/templates/report_import_scheduler.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: report-import-scheduler-6
  labels:
    app: report-import-scheduler-6
    chart: opticloud-0.1.0
    release: release-name
spec:
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  schedule: "0 */6 * * *"
  startingDeadlineSeconds: 100
  jobTemplate:
    metadata:
      labels:
        app: report-import-scheduler-6
        release: release-name
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            app: report-import-scheduler-6
            release: release-name
        spec:
          initContainers:
          - name: "wait-restapi"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
          containers:
          - name: report-import-scheduler-6
            image: "cleanelkdb:local"
            imagePullPolicy: Never
            env:
            - name: PERIOD
              value: "6"
            - name: CLUSTER_SECRET
              valueFrom:
                secretKeyRef:
                  name: cluster-secret
                  key: cluster_secret
            command:
            - "/bin/sh"
            - "-ec"
            - |
              set -x
              
              curl -X POST -f -H "Secret: $CLUSTER_SECRET" -d '{"period": '"$PERIOD"'}' \
                http://restapi:80/restapi/v2/schedule_imports
          restartPolicy: Never
---
# Source: opticloud/templates/resource_discovery.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: resource-discovery-scheduler
  labels:
    app: resource-discovery-scheduler
    chart: opticloud-0.1.0
    release: release-name
spec:
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  schedule: "*/5 * * * *"
  startingDeadlineSeconds: 100
  jobTemplate:
    metadata:
      labels:
        app: resource-discovery-scheduler
        release: release-name
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            app: resource-discovery-scheduler
            release: release-name
        spec:
          initContainers:
          - name: "wait-restapi"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
          containers:
          - name: resource-discovery-scheduler
            image: "resource_discovery:local"
            imagePullPolicy: Never
            command:
            - "python3"
            - "docker_images/resource_discovery/scheduler.py"
            env:
            - name: HX_ETCD_HOST
              value: etcd-client
            - name: HX_ETCD_PORT
              value: "2379"
          restartPolicy: Never
---
# Source: opticloud/templates/resource_observer.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: resource-observer-scheduler
  labels:
    app: resource-observer-scheduler
    chart: opticloud-0.1.0
    release: release-name
spec:
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  schedule: "*/5 * * * *"
  startingDeadlineSeconds: 100
  jobTemplate:
    metadata:
      labels:
        app: resource-observer-scheduler
        release: release-name
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            app: resource-observer-scheduler
            release: release-name
        spec:
          initContainers:
          - name: "wait-restapi"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
          containers:
          - name: resource-observer-scheduler
            image: "resource_observer:local"
            imagePullPolicy: Never
            command:
            - "python3"
            - "docker_images/resource_observer/scheduler.py"
            env:
            - name: HX_ETCD_HOST
              value: etcd-client
            - name: HX_ETCD_PORT
              value: "2379"
          restartPolicy: Never
---
# Source: opticloud/templates/resource_violations.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: resource-violations-scheduler
  labels:
    app: resource-violations-scheduler
    chart: opticloud-0.1.0
    release: release-name
spec:
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  schedule: "*/5 * * * *"
  startingDeadlineSeconds: 100
  jobTemplate:
    metadata:
      labels:
        app: resource-violations-scheduler
        release: release-name
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            app: resource-violations-scheduler
            release: release-name
        spec:
          initContainers:
          - name: "wait-restapi"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
          containers:
          - name: resource-violations-scheduler
            image: "resource_violations:local"
            imagePullPolicy: Never
            command:
            - "python3"
            - "docker_images/resource_violations/scheduler.py"
            env:
            - name: HX_ETCD_HOST
              value: etcd-client
            - name: HX_ETCD_PORT
              value: "2379"
          restartPolicy: Never
---
# Source: opticloud/templates/risp_scheduler.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: risp-scheduler
  labels:
    app: risp-scheduler
    chart: opticloud-0.1.0
    release: release-name
spec:
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  schedule: "45 */1 * * *"
  startingDeadlineSeconds: 100
  jobTemplate:
    metadata:
      labels:
        app: risp-scheduler
        release: release-name
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            app: risp-scheduler
            release: release-name
        spec:
          initContainers:
          - name: "wait-rabbitmq"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
          - name: "wait-etcd-client"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z etcd-client.default.svc.cluster.local 2379 -w 2; do sleep 2; done']
          - name: "wait-restapi"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
          containers:
          - name: risp-scheduler
            image: "risp_scheduler:local"
            imagePullPolicy: Never
            env:
            - name: IMAGE_ID
              value: 
            - name: HX_ETCD_HOST
              value: etcd-client
            - name: HX_ETCD_PORT
              value: "2379"
          restartPolicy: Never
---
# Source: opticloud/templates/thanos_compactor.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: thanos-compactor
  labels:
    app: thanos-compactor
    release: release-name
spec:
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 2
  schedule: "0 */12 * * *"
  startingDeadlineSeconds: 100
  suspend: false
  jobTemplate:
    metadata:
      labels:
        app: thanos-compactor
        release: release-name
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            app: thanos-compactor
            release: release-name
        spec:
          initContainers:
          - name: "wait-minio"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z minio.default.svc.cluster.local 80 -w 2; do sleep 2; done']
          containers:
          - name: compact
            image: "thanosio/thanos:v0.31.0"
            imagePullPolicy: IfNotPresent
            command:
              - thanos
            args:
              - compact
              - --log.level=info
              - --http-address=0.0.0.0:10902
              - --consistency-delay=4h
              - --objstore.config-file=/var/thanos/config/thanos_conf.yaml
              - --data-dir=/var/thanos/data
              - --delete-delay=6h
              - --retention.resolution-raw=30d
              - --retention.resolution-5m=90d
              - --retention.resolution-1h=120d
              - --compact.enable-vertical-compaction
            ports:
              - containerPort: 10902
                name: http
                protocol: TCP
            livenessProbe:
              httpGet:
                path: /-/healthy
                port: http
              initialDelaySeconds: 30
              timeoutSeconds: 30
              periodSeconds: 10
              successThreshold: 1
              failureThreshold: 6
            readinessProbe:
              httpGet:
                path: /-/ready
                port: http
              initialDelaySeconds: 30
              timeoutSeconds: 30
              periodSeconds: 10
              successThreshold: 1
              failureThreshold: 6
            volumeMounts:
              - name: data
                mountPath: /var/thanos/data
              - name: config
                mountPath: /var/thanos/config
                readOnly: true
          volumes:
            - name: config
              secret:
                secretName: thanos-secret
                items:
                  - key: thanos_conf.yaml
                    path: thanos_conf.yaml
            - name: data
              persistentVolumeClaim:
                claimName: thanos-compactor-claim
          restartPolicy: Never
---
# Source: opticloud/templates/trapper_scheduler.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: trapper-scheduler
  labels:
    app: trapper-scheduler
    chart: opticloud-0.1.0
    release: release-name
spec:
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  schedule: "45 */1 * * *"
  startingDeadlineSeconds: 100
  jobTemplate:
    metadata:
      labels:
        app: trapper-scheduler
        release: release-name
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            app: trapper-scheduler
            release: release-name
        spec:
          initContainers:
          - name: "wait-rabbitmq"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z rabbitmq.default.svc.cluster.local 5672 -w 2; do sleep 2; done']
          - name: "wait-etcd-client"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z etcd-client.default.svc.cluster.local 2379 -w 2; do sleep 2; done']
          - name: "wait-restapi"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z restapi.default.svc.cluster.local 80 -w 2; do sleep 2; done']
          containers:
          - name: trapper-scheduler
            image: "trapper_scheduler:local"
            imagePullPolicy: Never
            env:
            - name: IMAGE_ID
              value: 
            - name: HX_ETCD_HOST
              value: etcd-client
            - name: HX_ETCD_PORT
              value: "2379"
          restartPolicy: Never
---
# Source: opticloud/templates/users_dataset_generator.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: users-dataset-generator
  labels:
    app: users-dataset-generator
    chart: opticloud-0.1.0
    release: release-name
spec:
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  schedule: "0 0 * * *"
  startingDeadlineSeconds: 100
  jobTemplate:
    metadata:
      labels:
        app: users-dataset-generator
        release: release-name
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            app: users-dataset-generator
            release: release-name
        spec:
          initContainers:
          - name: "wait-mariadb"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z mariadb.default.svc.cluster.local 3306 -w 2; do sleep 2; done']
          - name: "wait-clickhouse"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z clickhouse.default.svc.cluster.local 9000 -w 2; do sleep 2; done']
          - name: "wait-mongo"
            image: "busybox:1.30.0"
            imagePullPolicy: IfNotPresent
            command: ['sh', '-c', 'until nc -z mongo.default.svc.cluster.local 80 -w 2; do sleep 2; done']
          containers:
          - name: users-dataset-generator
            image: "users_dataset_generator:local"
            imagePullPolicy: Never
            env:
            - name: IMAGE_ID
              value: 
            - name: HX_ETCD_HOST
              value: etcd-client
            - name: HX_ETCD_PORT
              value: "2379"
          restartPolicy: Never
---
# Source: opticloud/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/custom-http-errors: "503"
    nginx.ingress.kubernetes.io/default-backend: error-pages
    nginx.ingress.kubernetes.io/proxy-body-size: "512m"
    nginx.ingress.kubernetes.io/connection-proxy-header: "keep-alive"
    nginx.ingress.kubernetes.io/client-body-buffer-size:  "512m"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "600"
    
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    
  name: opticloud
  namespace: default
spec:
  rules:
  - http:
      paths:
      - backend:
          serviceName: auth
          servicePort: 80
        path: /auth
      - backend:
          serviceName: restapi
          servicePort: 80
        path: /restapi
      - backend:
          serviceName: keeper
          servicePort: 80
        path: /report
      - backend:
          serviceName: ngui
          servicePort: 80
        path: /
      - backend:
          serviceName: herald
          servicePort: 80
        path: /herald
      - backend:
          serviceName: katara
          servicePort: 80
        path: /katara
      - backend:
          serviceName: insider-api
          servicePort: 80
        path: /insider
      - backend:
          serviceName: slacker
          servicePort: 80
        path: /slacker
      - backend:
          serviceName: metroculusapi
          servicePort: 80
        path: /metroculus
      - backend:
          serviceName: jira-bus
          servicePort: 80
        path: /jira_bus
      - backend:
          serviceName: jira-ui
          servicePort: 80
        path: /jira_ui
      - backend:
          serviceName: diproxy
          servicePort: 80
        path: /storage
      - backend:
          serviceName: pharos-receiver
          servicePort: 80
        path: /pharos_receiver
      - backend:
          serviceName: ohsu
          servicePort: 80
        path: /ohsu
      - backend:
          serviceName: arcee
          servicePort: 80
        path: /arcee
      - backend:
          serviceName: bulldozer-api
          servicePort: 80
        path: /bulldozer
  tls:
  - secretName: defaultcert
